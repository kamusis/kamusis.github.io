{"meta":{"title":"Kamus' Notes","subtitle":"面朝大海，春暖花开","description":"Expert insights and practical tips on openGauss, MogDB, Oracle, Linux, and cloud-native technologies. Elevate your database management and system administration skills with in-depth technical articles, tutorials, and best practices for real-world challenges.","author":"Kamus","url":"http://kamusis.github.io","root":"/"},"pages":[{"title":"About me","date":"2024-05-27T08:06:25.000Z","updated":"2024-05-31T16:23:56.520Z","comments":true,"path":"about/index.html","permalink":"http://kamusis.github.io/about/index.html","excerpt":"","text":"2006 年中回家省亲，恰逢大一新生入学， 家乡车站上挂着横幅迎接祖国各地到来的莘莘学子。 我拖着箱子走出车站， 迎接新生的同学看着我说－这位同学，你…… 大汗，我已经不做学生很多年了。 其实我没那么看上去那么小， 说实话我都该要到隐瞒年龄的阶段了。 闲话少说，以下为个人简历， 省略学习经历 1500 字…… 省略工作经历 3000 字…… 我，其实是一个 DBA。 我，其实是一个 Consultant。 2007 年被提名为 Oracle ACE，2011 年被提名为 Oracle ACE Director。 我是 Oracle8i&#x2F;9i&#x2F;10g OCP，不是 OCM，估计也永远不会去考 OCM。 我是ACOUG成员，实际上也是 ACOUG 创办者以及管理者。 现在我在云和恩墨（北京）信息技术有限公司任战略市场 云服务事业部总经理 副总经理一职，仍然有不少时间从事技术工作。 可以通过以下途径联系我，但是，数据库问题最好能够在墨天轮 DBASK上提问。 我的邮件地址： 我的 Google Talk： 我的 Linkedin： Also, you can find my resume in English(pdf) from here. 真人 show，某天的手机自拍。 我们比你们多一个世界。 2005 年，我在魔兽世界的第一个角色，盗贼日蚀，60 级，持残忍利刃和血腥撕裂者，那时候血牙头还很土。那时候的 WoW 还很好玩。 2006 年，盗贼日蚀，60 级，持上古其拉撕裂者和伊普利斯.堕落炽天使之刃，仍然是血牙头。基本上是那个时期盗贼的顶级装备。那是玩的最疯的时候。 2007 年，厌倦了 40 人上班式的 RAID，从国服 AFK，转战台服，亡灵术士，D3 套，70 级。实际上是一个人在台服玩，有一搭没一搭的。 2007 年 9 月，国服开 TBC，于是从台服 AFK，回归国服，reroll 了血精灵猎人，部分 S2，70 级。玩到 2008 年，也只是偶尔去打一下卡拉赞而已，高端副本基本没有进去过，AFK。 2008 年 11 月，台服开放 WLK 巫妖王之怒。2008 年底再次回到台服，2009 年 11 月 21 日，防骑+奶骑，2T8.5+2T9，目前 3.2 版本，可坦 25 人十字军。80 级。有一帮最早 60 级时在国服的朋友，倒也其乐融融。 闲暇时间里，把之前 70 级的术士升级到了 80 级。2009 年 11 月 21 日，术士 Neggo，2T8+2T9，80 级。休闲小号，主城木桩 DPS 不到 4000。 2010 年 6 月 27，时隔半年，术士 Neggo，4T10。主城木桩 DPS 到 5600。 2011 年，台服魔兽世界已经升级到 CTM 大灾变，国服在经历了从 2007 年 9 月份一直到 2010 年 8 月整整三年的 TBC 之后极为艰难地开放了 WLK 巫妖王之怒，现在依旧杯具地落后全世界一整个版本（其间在大陆的代理商从九城易手到网易，也是风波迭起）。可惜无论是 CTM 还是 WLK，自己对于这款游戏的热情已经降到历史最低点，仅仅把台服的圣骑号升级到 85，打过几次 5 人副本，连 5 人 H 本都没有进过，更别提 10 人副本了。也许作为一款单机游戏，练练小号四处逛逛，也还不错。 2015 年，魔兽世界 10 周年了，已经升级到 6.0 版本，我还在玩这个游戏，宠物对战、帐号共享坐骑、全新的要塞系统，人物等级提升到 100 级。我在台服。Reroll 了死骑，叮到 90 级，然后练到 100 级，5H 毕业。 2018 年，魔兽世界升级到了 8.0 版本，嗯，我还在玩。从 7.0 版本的破碎群岛到 7.3 版本的阿古斯，脚男们干翻了基尔加丹，又传送到被蛋总折跃到艾泽拉斯旁边的阿古斯上去，进攻燃烧军团的老家，最后在万神殿的一大堆泰坦们的帮助下，推倒了萨格拉斯。萨格拉斯临了孤注一掷，一剑插进了艾泽拉斯，这大家伙在希利苏斯像一座山一样矗立着，而且还开始侵蚀艾泽拉斯之心。然后就开了 8.0，部落和联盟在 7.0 里面合作的太久了，像一对好基友一样卿卿我我，然而部落新任大领主希尔瓦娜斯觉得这肯定不是个事儿，与其以后大家都有了艾泽里特发展出牛逼武器，然后斗个大家都死，不如现在就开打。于是，部落烧了泰达希尔，联盟围攻了幽暗城，开始了艾泽拉斯争霸。 回归国服，Reroll 了德鲁伊，叮到 110 级，练到 120 级，秒上坐骑，还是挺爽的。为了部落！[ 整个 8.0 阶段并没有正经玩，时间很快来到 2020 年底，魔兽世界开了 9.0 暗影国度，由于之前经历了 8 个版本，无论是人物属性还是伤害数值已经呈现爆炸形式，因此 9.0 版本，暴雪对魔兽世界进行了等级压缩，8.0 版本的满级 120 级人物在进入 9.0 之后会变成 50 级。9.0 允许从 50 级开始再次升级到满级 60 级。 在 9.0 版本里，部落大酋长希尔瓦娜斯反复横跳，黑化又洗白，女王在 8.0 火烧泰达希尔之后，又在 9.0 喊出了“部落就是废物”。总之，剧情稀碎。但是有一说一，暗影界的美工是真不错，尤其是晋升堡垒，初进地图着实惊艳。 2023 年 1 月 24 日，由于暴雪和网易的纠葛，在中国大陆运营了 18 年的魔兽世界正式关停，魔兽玩家哀鸿遍野。 再次转战台服，2023 年 11 月 29 日，10.0 版本巨龙时代正式上线。人物等级上限 70 级。 今年 2024 年，年底魔兽世界即将进入 11.0 地心之战。而明年将会是魔兽世界 20 周年。"}],"posts":[{"title":"How to Generate a VSIX File from VS Code Extension Source Code","slug":"How-to-Generate-a-VSIX-File-from-VS-Code-Extension-Source-Code","date":"2024-12-12T06:39:52.000Z","updated":"2024-12-12T06:42:56.299Z","comments":true,"path":"2024/12/12/How-to-Generate-a-VSIX-File-from-VS-Code-Extension-Source-Code/","permalink":"http://kamusis.github.io/2024/12/12/How-to-Generate-a-VSIX-File-from-VS-Code-Extension-Source-Code/","excerpt":"","text":"As I’ve been using Windsurf as my primary code editor, I encountered a situation where the vs-picgo extension wasn’t available in the Windsurf marketplace. This necessitated the need to manually package the extension from its source code. This guide documents the process of generating a VSIX file for VS Code extensions, which can then be installed manually in compatible editors like Windsurf. In this guide, I’ll walk you through the process of generating a VSIX file from a VS Code extension’s source code. We’ll use the popular vs-picgo extension as an example. PrerequisitesBefore we begin, make sure you have the following installed: Node.js (version 12 or higher) npm (comes with Node.js) Step 1: Install Required ToolsFirst, we need to install two essential tools: yarn: A package manager that will handle our dependencies vsce: The VS Code Extension Manager tool that creates VSIX packages 12345# Install Yarn globallysudo npm install -g yarn# Install vsce globallysudo npm install -g @vscode/vsce Step 2: Prepare the Project Clone or download the extension source code: 12git clone https://github.com/PicGo/vs-picgo.gitcd vs-picgo Install project dependencies: 1yarn install This command will: Read the package.json file Install all required dependencies Create or update the yarn.lock file Note: The yarn.lock file is important! Don’t delete it as it ensures consistent installations across different environments. Step 3: Build the ExtensionBuild the extension using the production build command: 1yarn build:prod This command typically: Cleans the previous build output Compiles TypeScript&#x2F;JavaScript files Bundles all necessary assets Creates the dist directory with the compiled code In vs-picgo’s case, the build process: Uses esbuild for fast bundling Creates both extension and webview bundles Generates source maps (disabled in production) Optimizes the code for production use Step 4: Package the ExtensionFinally, create the VSIX file: 1vsce package This command: Runs any pre-publish scripts defined in package.json Validates the extension manifest Packages all required files into a VSIX file Names the file based on the extension’s name and version (e.g., vs-picgo-2.1.6.vsix) The resulting VSIX file will contain: Compiled JavaScript files Assets (images, CSS, etc.) Extension manifest Documentation files License information What’s Inside the VSIX?The VSIX file is essentially a ZIP archive with a specific structure. For vs-picgo, it includes: 123456789101112131415vs-picgo-2.1.6.vsix├─ [Content_Types].xml ├─ extension.vsixmanifest └─ extension/ ├─ LICENSE.txt ├─ changelog.md ├─ logo.png ├─ package.json ├─ package.nls.json ├─ readme.md └─ dist/ ├─ extension.js └─ webview/ ├─ index.css └─ index.js Installing the ExtensionYou can install the generated VSIX file in VS Code or any compatible editor by: Opening VS Code&#x2F;Windsurf&#x2F;Cursor … Going to the Extensions view Clicking the “…” menu (More Actions) Selecting “Install from VSIX…” Choosing your generated VSIX file TroubleshootingIf you encounter any issues: Missing dist directory error: This is normal on first build The build process will create it automatically Dependency errors: Run yarn install again Check if you’re using the correct Node.js version VSIX packaging fails: Verify your package.json is valid Ensure all required files are present Check the extension manifest for errors ConclusionBuilding a VS Code extension VSIX file is straightforward once you have the right tools installed. The process mainly involves installing dependencies, building the source code, and packaging everything into a VSIX file. Remember to keep your yarn.lock file and always build in production mode before packaging to ensure the best performance and smallest file size for your users. Happy extension building! 🚀","categories":[],"tags":[{"name":"VS Code","slug":"VS-Code","permalink":"http://kamusis.github.io/tags/VS-Code/"},{"name":"VSIX","slug":"VSIX","permalink":"http://kamusis.github.io/tags/VSIX/"},{"name":"Extension","slug":"Extension","permalink":"http://kamusis.github.io/tags/Extension/"},{"name":"Packaging","slug":"Packaging","permalink":"http://kamusis.github.io/tags/Packaging/"},{"name":"Windsurf","slug":"Windsurf","permalink":"http://kamusis.github.io/tags/Windsurf/"}]},{"title":"What is DBOS and What Should We Expect","slug":"What-is-DBOS-and-What-Should-We-Expect","date":"2024-11-14T03:38:12.000Z","updated":"2024-11-19T17:26:33.523Z","comments":true,"path":"2024/11/14/What-is-DBOS-and-What-Should-We-Expect/","permalink":"http://kamusis.github.io/2024/11/14/What-is-DBOS-and-What-Should-We-Expect/","excerpt":"","text":"IntroductionThe computing world is witnessing a paradigm shift in how we think about operating systems. A team of researchers has proposed DBOS (Database-Oriented Operating System), a radical reimagining of operating system architecture that puts data management at its core. But what exactly is DBOS, and why should we care? What is DBOS?DBOS is a novel operating system architecture that treats data management as its primary concern rather than traditional OS functions like process management and I&#x2F;O. The key insight behind DBOS is that modern applications are increasingly data-centric, yet our operating systems still follow designs from the 1970s that prioritize computation over data management. Instead of treating databases as just another application, DBOS makes database technology the fundamental building block of the operating system itself. This means core OS functions like process scheduling, resource management, and system monitoring are implemented using database principles and technologies. Who is behind DBOS?DBOS is a collaborative research project involving almost twenty researchers across multiple institutions including MIT, Stanford, UW-Madison, Google, VMware, and other organizations. The project is notably led by database pioneer Michael Stonebraker, who is an ACM Turing Award winner (2014) and Professor Emeritus at UC Berkeley, currently affiliated with MIT. Key institutions and researchers involved include: MIT: Michael Stonebraker, Michael Cafarella, Çağatay Demiralp, and others Stanford: Matei Zaharia, Christos Kozyrakis, and others UW-Madison: Xiangyao Yu Industry partners: Researchers from Google, VMware, and other organizations The people behind DBOS can be found at DBOS Project. Ultimate GoalThe ultimate goal of DBOS is to create an operating system that is data-centric and data-driven, the OS is on top of a DBMS, not like today’s DBMS on top of the OS.ALL system data should reside in the DBMS. Replace the “everything is a file” mantra with “everything is a table” All system state and metadata stored in relational tables All changes to OS state should be through database transactions DBMS provides All functions that a DBMS can do, for example, files are blobs and tables in the DBMS. SQL-based interface for both application and system data access To achieve very high performance, the DBMS must leverage sophisticated caching and parallelization strategies and compile repetitive queries into machine code. Benefits Strong Security and Privacy Native GDPR compliance through data-centric design Attribute-based access control (ABAC) Complete audit trails and data lineage Privacy by design through unified data management Fine-grained access control at the data level Enhanced monitoring and threat detection Simplified compliance with regulatory requirements Built-in data encryption and protection mechanisms Enhanced Performance and Efficiency Optimized resource allocation through database-driven scheduling Reduced data movement and copying Better cache utilization through database techniques Intelligent workload management Advanced query optimization for system operations Improved resource utilization through data-aware decisions Reduced system overhead through unified architecture Better support for modern hardware architectures Improved Observability and Management Comprehensive system-wide monitoring Real-time analytics on system performance Easy troubleshooting through SQL queries Better capacity planning capabilities Unified logging and debugging interface Historical analysis of system behavior Predictive maintenance capabilities Simplified system administration Advanced Application Support Native support for distributed applications Better handling of microservices architecture Simplified state management Enhanced support for modern cloud applications Built-in support for data-intensive applications Improved consistency guarantees Better transaction management Simplified development of distributed systems Technical ImplementationDBOS proposes implementing core OS functions using database principles: Process Management: Processes and their states managed as database tables Resource Scheduling: SQL queries and ML for intelligent scheduling decisions System Monitoring: Metrics collection and analysis through database queries Security: Access control and auditing via database mechanisms Storage: File system metadata stored in relational tables Networking: Network state and routing managed through database abstractions What Should We Expect?Near-term Impact Proof of Concept: The researchers are working on demonstrating DBOS’s capabilities through specific use cases like log processing and accelerator management. Performance Improvements: Early implementations might show significant improvements in data-intensive workloads. Development Tools: New tools and frameworks that leverage DBOS’s database-centric approach. Long-term PossibilitiesCloud Native Integration: DBOS could become particularly relevant for cloud computing environments where data management is crucial. AI&#x2F;ML Operations: Better support for AI and machine learning workloads through intelligent resource management. Privacy-First Computing: A new standard for building privacy-compliant systems from the ground up. Challenges AheadSeveral technical and practical challenges need to be addressed: Performance Minimizing database overhead for system operations Optimizing query performance for real-time OS operations Efficient handling of high-frequency system events Compatibility Supporting existing applications and system calls Maintaining POSIX compliance where needed Migration path for legacy systems Distributed Systems Maintaining consistency across distributed nodes Handling network partitions and failures Scaling database operations across clusters Adoption Convincing stakeholders to adopt radical architectural changes Training developers in the new paradigm Building an ecosystem of compatible tools and applications ConclusionDBOS represents a bold reimagining of operating system design for the data-centric world. While it’s still in its early stages, the potential benefits for security, privacy, and developer productivity make it an exciting project to watch. As data continues to grow in importance, DBOS’s approach might prove to be prescient.The success of DBOS will largely depend on how well it can demonstrate its advantages in real-world scenarios and whether it can overcome the inherent challenges of introducing such a fundamental change to system architecture. For developers, system administrators, and anyone interested in the future of computing, DBOS is definitely worth keeping an eye on.Whether DBOS becomes the next evolution in operating systems or remains an interesting academic exercise, its ideas about putting data management at the center of system design will likely influence future OS development. References M. Stonebraker et al., “DBOS: A DBMS-oriented Operating System,” Proceedings of the VLDB Endowment, Vol. 15, No. 1, 2021.https://www.vldb.org/pvldb/vol15/p21-stonebraker.pdf DBOS Project Official Website, MIT CSAIL and Stanford University.https://dbos-project.github.io/ X. Yu et al., “A Case for Building Operating Systems with Database Technology,” Proceedings of the 12th Conference on Innovative Data Systems Research (CIDR ‘22), 2022.https://www.cidrdb.org/cidr2022/papers/p82-yu.pdf DBOS Research Group Publications.https://dbos-project.github.io/papers/ These references provide detailed technical information about DBOS’s architecture, implementation, and potential impact on the future of operating systems. The papers discuss various aspects from system design to performance evaluation, security considerations, and practical applications.","categories":[],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://kamusis.github.io/tags/PostgreSQL/"},{"name":"Database","slug":"Database","permalink":"http://kamusis.github.io/tags/Database/"},{"name":"DBOS","slug":"DBOS","permalink":"http://kamusis.github.io/tags/DBOS/"},{"name":"Operating System","slug":"Operating-System","permalink":"http://kamusis.github.io/tags/Operating-System/"}]},{"title":"Some Misconceptions About Database Performance - Application Concurrent Requests, Database Connections, and Database Connection Pools","slug":"Some-Misconceptions-About-Database-Performance-Application-Concurrent-Requests-Database-Connections-and-Database-Connection-Pools","date":"2024-08-21T08:44:47.000Z","updated":"2024-08-21T08:46:33.645Z","comments":true,"path":"2024/08/21/Some-Misconceptions-About-Database-Performance-Application-Concurrent-Requests-Database-Connections-and-Database-Connection-Pools/","permalink":"http://kamusis.github.io/2024/08/21/Some-Misconceptions-About-Database-Performance-Application-Concurrent-Requests-Database-Connections-and-Database-Connection-Pools/","excerpt":"","text":"Let’s start with a chart from a database stress test result. In the chart above, the X-axis represents the number of concurrent connections, and the Y-axis represents the TPS (Transactions Per Second) provided by the database. The stress testing tool used is pgbench, and the database is a single instance of PostgreSQL 16. TPS_PGDirect represents the TPS when pgbench directly sends the load to PostgreSQL, while TPS_PGBouncer represents the TPS when pgbench sends the load to the PGBouncer database connection pool. We can clearly see that when the number of connections exceeds 256, the TPS begins to gradually decline when the load is sent directly to the database. When the number of connections eventually rises to over 8000, the database is almost at a standstill. However, when using PGBouncer, the TPS remains stable and excellent, with the database consistently providing high TPS regardless of the number of connections. Many people might conclude from this chart that if the concurrent connections increase beyond a certain threshold, PostgreSQL becomes inadequate, and PGBouncer must be used to improve performance. Some users may attempt to conduct various hardware environment combination tests to derive a reference threshold number to guide at what pressure PostgreSQL can cope, and once it exceeds a certain pressure, PGBouncer should be enabled. So, is this conclusion correct? And does testing out a threshold number have real guiding significance? Understanding Concurrent PerformanceTo determine how many concurrent business requests a database can handle, we should focus on the Y-axis from the test chart above, rather than the X-axis. The Y-axis TPS represents the number of transactions the database can process per second, while the X-axis connection count is merely the database connection number we configured ourselves (which can be simply understood as the max_connections parameter value). We can observe that at 256 connections, the direct database stress test with pgbench allows the database to provide over 50,000 TPS, which is actually almost the highest number in the entire stress test (WITHOUT PGBouncer). Subsequent tests using PGBouncer, this number was not significantly exceeded. Therefore, PGBouncer is not a weapon to improve database performance. What is performance? Performance means: for a determined business transaction pressure (how much work a business transaction needs to handle, such as how much CPU, memory, and IO it consumes), the more transactions the database can execute, the higher the performance. In the chart above, this is represented by the Y-axis TPS (Transactions Per Second). For example, suppose a user request (Application Request) requires executing 5 transactions from start to finish, and assume these transactions consume similar amounts of time (even if there are differences, we can always get the average transaction runtime in a system). In this case, 50,000 TPS means the database can handle 10,000 user requests per second, which can be simply understood as handling 10,000 concurrent users. If TPS rises to 100,000, the database can handle 20,000 concurrent users; if TPS drops to 10,000, it can only handle 2,000 concurrent users. This is a simple calculation logic, and in this description, we did not mention anything about database connections (User Connections). So why does it seem like PGBouncer improves performance when the number of connections continues to increase? Because in this situation, PGBouncer acts as a limiter on database connections, reducing the extra overhead caused by the database&#x2F;operating system handling too many connections, allowing the database and operating system to focus on processing the business requests sent from each connection, and enabling user requests exceeding the connection limit to share database connections. What PGBouncer does is: if your restaurant can only provide 10 tables, then I only allow 10 tables of guests to enter, and other guests have to wait outside. However, why should we let 100 tables of guests rush into the restaurant all at once? Without PGBouncer, can’t we control allowing only 10 tables of guests to enter? In modern applications, application-side connection pools are more commonly used, such as connection pools in various middleware software (Weblogic, WebSphere, Tomcat, etc.) or dedicated JDBC connection pools like HikariCP. Exceeding the number of connections that can be handled is unnecessary. When the database is already at its peak processing capacity, having more connections will not improve overall processing capability; instead, it will slow down the entire database. To summarize the meaning of the chart above more straightforwardly: when the total number of database connections is at 256, the database reaches its peak processing capacity, and direct database connections can provide 50,000 TPS without any connection pool. If 50,000 TPS cannot meet the application’s concurrent performance requirements, increasing the number of connections will not help. But if connections are increased (wrongly, without any other control), a layer of connection pool like PGBouncer can be added as a connection pool buffer to keep performance stable at around 50,000 TPS. So, do we have any reason to arbitrarily increase the number of connections? No. What we need to do is: allow the database to open only the number of connections within its processing capacity and enable these connections to be shared by more user requests. When the restaurant size is fixed, the higher the turnover rate, the higher the restaurant’s daily revenue. How to Ask Questions About PerformanceInstead of asking: how can we ensure the database provides sufficient TPS with 4000 connections, we should ask: how can we ensure the database provides 50,000 TPS. Because the number of connections is not a performance metric, it is merely a parameter configuration option. In the example above, a connection count of 4000 or 8000 is not a fixed value, nor is it an optimized value. We cannot try to do more additional database optimizations on an unoptimized, modifiable parameter setting, as this may lead to a reversal of priorities. If we can already provide maximum processing capacity with 256 fully loaded database connections, why optimize a database with more than 256 connections? We should ensure the database always runs at full capacity with a maximum of 256 connections. Now, our question about performance can be more specific. Suppose we have multiple sets of database servers with different configurations; what should the fully loaded connection count be for each set to achieve optimal processing capacity? This requires testing. However, we will ultimately find that this fully loaded connection count is usually related only to the database server’s hardware resources, such as the number of CPU cores, CPU speed, memory size, storage speed, and the bandwidth and latency between storage and host. Is Database Application Performance Only Related to Hardware Resources?Of course not. Concurrent performance (P) &#x3D; Hardware processing capability (H) &#x2F; Business transaction pressure (T). As mentioned earlier, the performance metric TPS is for a “determined business transaction pressure,” meaning a determined T value. When the T value is determined, we can only improve the H value to further enhance the P value. Conversely, there is naturally another optimization approach: reducing the T value. There are various ways to reduce the T value, such as adding necessary indexes, rewriting SQL, using partitioned tables, etc. The optimization plan should be formulated based on where the T value is consumed most, which is another topic. ConclusionTo answer the two initial questions: If concurrent connections increase beyond a certain threshold, PostgreSQL becomes inadequate, and PGBouncer must be used to improve performance. Is this conclusion correct? If we must create more connections to the database than the hardware processing capacity for certain hardware resources, using PGBouncer can stabilize performance. However, we have no reason to create more connections than the hardware processing capacity. We have various means to control the maximum number of connections the database can open, and various middleware solutions can provide connection pools. From this perspective, PGBouncer does not improve database performance; it merely reduces concurrency conflicts, doing what any connection pool software can do. Is it meaningful to attempt various hardware environment combination tests to derive a reference threshold number to guide at what pressure PostgreSQL can cope, and once it exceeds a certain pressure, PGBouncer should be enabled? It has some significance, but simple stress tests like pgbench are not enough to guide real applications. Concurrent performance (P) &#x3D; Hardware processing capability (H) &#x2F; Business transaction pressure (T). Testing in a determined hardware environment means the H value is fixed, so we should focus on T during testing. The T value provided by pgbench is a very simple transaction pressure (only four tables, simple CRUD operations), and the P value tested under this pressure can only be used as a reference. To guide application deployment, typical transaction pressure of the application itself must be abstracted for testing to have guiding significance.","categories":[],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://kamusis.github.io/tags/PostgreSQL/"},{"name":"Database","slug":"Database","permalink":"http://kamusis.github.io/tags/Database/"},{"name":"Performance","slug":"Performance","permalink":"http://kamusis.github.io/tags/Performance/"}]},{"title":"Flashback Features in MogDB","slug":"Flashback-Features-in-MogDB","date":"2024-06-15T09:32:07.000Z","updated":"2024-06-15T09:32:07.780Z","comments":true,"path":"2024/06/15/Flashback-Features-in-MogDB/","permalink":"http://kamusis.github.io/2024/06/15/Flashback-Features-in-MogDB/","excerpt":"","text":"What is Database FlashbackThe flashback capability of a database allows users to query the contents of a table at a specific point in the past or quickly restore a table or even the entire database to a time before an erroneous operation. This feature can be extremely useful, and sometimes even a lifesaver. Database flashback capabilities are generally divided into three levels: Row-level Flashback: Also known as flashback query, this typically involves using a SELECT statement to retrieve data from a table as it was at a specific point in time, such as right before a DELETE command was issued. Table-level Flashback: Also known as flashback table, this usually involves using specialized DDL statements to recreate a table from the recycle bin. This is often used to recover from a TRUNCATE TABLE or DROP TABLE operation. Database-level Flashback: Also known as flashback database, this involves using specialized DDL statements to restore the entire database to a previous point in time. Unlike PITR (Point-In-Time Recovery) which involves restoring from backups, flashback database is faster as it does not require restoring the entire database from backup sets. In MogDB version 5.0, only flashback query and flashback table have been implemented so far. Flashback database has not yet been implemented (Oracle has supported flashback database since version 10g). ScenarioImagine a regrettable scenario (Note: do not run the DELETE SQL command in a production environment). You have a table that records account names and account balances. 12345678MogDB=# select * from accounts; name | amount------+-------- A | 100 B | 100 C | 100 D | 99(4 rows) You intended to execute an SQL command to delete account records with a balance of 99 units. Normally, this should delete one record. To demonstrate, let’s use SELECT instead of DELETE. 12345MogDB=# select * from accounts where amount=99; name | amount------+-------- D | 99(1 row) However, the minus sign “-“ and the equals sign “&#x3D;” are adjacent on the keyboard, and you have fat fingers, so you accidentally pressed the minus sign. As a result, the command you sent to the database looks like this: 1delete from accounts where amount-99; For demonstration purposes, we’ll still use SELECT instead of DELETE. 1234567MogDB=# select * from accounts where amount-99; name | amount------+-------- A | 100 B | 100 C | 100(3 rows) A terrifying thing happens: all data except the record with a balance exactly equal to 99 is returned. This means that if this were the DELETE command mentioned above, you would have deleted all accounts with a balance not equal to 99. The good news is that starting from MogDB version 3.0, validation for such dangerous syntax involving the minus sign has been added. Now, executing the same SQL will result in an error. 1234MogDB=# delete from accounts where amount-99;ERROR: argument of WHERE must be type boolean, not type integerLINE 1: delete from accounts where amount-99; ^ However, in the community editions of openGauss, MySQL, or MariaDB, such dangerous syntax can still be executed normally. 12345678910111213141516171819202122gsql ((openGauss 6.0.0-RC1 build ed7f8e37) compiled at 2024-03-31 12:41:30 commit 0 last mr )Non-SSL connection (SSL connection is recommended when requiring high-security)Type &quot;help&quot; for help.openGauss=# select version(); version------------------------------------------------------------------------------------------------------------------------------------------------------------ (openGauss 6.0.0-RC1 build ed7f8e37) compiled at 2024-03-31 12:41:30 commit 0 last mr on aarch64-unknown-linux-gnu, compiled by g++ (GCC) 10.3.1, 64-bit(1 row)openGauss=# create table accounts (name varchar2, amount int);CREATE TABLEopenGauss=# insert into accounts values (&#x27;A&#x27;,100),(&#x27;B&#x27;,100),(&#x27;C&#x27;,100),(&#x27;D&#x27;,99);INSERT 0 4openGauss=# delete from accounts where amount-99;DELETE 3openGauss=# select * from accounts; name | amount------+-------- D | 99(1 row) No matter what kind of erroneous operation occurs, suppose a data deletion error really happens. In MogDB, you still have the flashback feature available for recovery. Flashback Feature in MogDBThe flashback feature and its related implementations have undergone some changes since MogDB version 3.0. Applicable Only to Ustore Storage Engine: The flashback feature only works for tables using the ustore storage engine. The default astore no longer supports flashback queries. Therefore, you need to set enable_ustore=on. This parameter is off by default, and changing it requires a database restart to take effect. 123MogDB=# alter system set enable_ustore=on;NOTICE: please restart the database for the POSTMASTER level parameter to take effect.ALTER SYSTEM SET Setting undo_retention_time: This parameter specifies the retention time for old version data in the rollback segment, equivalent to the allowable time span for flashback queries. The default value is 0, meaning any flashback query will encounter a “restore point not found” error. Changing this parameter does not require a database restart. 12MogDB=# alter system set undo_retention_time=86400; -- 86400 seconds = 24 hoursALTER SYSTEM SET Enabling Database Recycle Bin for Truncate or Drop Operations: To flashback a table from a truncate or drop operation, you need to enable the database recycle bin by setting enable_recyclebin=on. This parameter is off by default, and changing it does not require a database restart. 12MogDB=# alter system set enable_recyclebin=on;ALTER SYSTEM SET Creating and Populating the Ustore TableCreate a ustore-based accounts table and insert some test data. 12345MogDB=# create table accounts (name varchar2, amount int) with (storage_type=ustore);CREATE TABLEMogDB=# insert into accounts values (&#x27;A&#x27;,100),(&#x27;B&#x27;,100),(&#x27;C&#x27;,100),(&#x27;D&#x27;,99);INSERT 0 4 Simulating an Erroneous DeletionNow, due to some erroneous operation, you delete all account records with amounts not equal to 99. 12345678MogDB=# delete from accounts where amount&lt;&gt;99;DELETE 3MogDB=# select * from accounts; name | amount------+-------- D | 99(1 row) Flashback QueryWhen you realize the mistake, it might be 1 minute or 1 hour later. As long as it is within 24 hours (due to the undo_retention_time setting), you can recover the data. Check the current timestamp and estimate the timestamp at the time of the erroneous operation. For simplicity, let’s assume you noted the system’s timestamp before issuing the delete command. 12345MogDB=# select sysdate; sysdate--------------------- 2024-06-13 18:40:18(1 row) Issue a flashback query to retrieve the data as it was at the specified timestamp. 12345678MogDB=# select * from accounts timecapsule TIMESTAMP to_timestamp(&#x27;2024-06-13 18:40:18&#x27;,&#x27;YYYY-MM-DD HH24:MI:SS&#x27;); name | amount------+-------- A | 100 B | 100 C | 100 D | 99(4 rows) Recovering DataYou can recover the data by creating a temporary table with the flashback query results and then inserting the data back into the original table. 12345678910111213141516171819202122MogDB=# create table tmp_accounts as select * from accounts timecapsule TIMESTAMP to_timestamp(&#x27;2024-06-13 18:40:18&#x27;,&#x27;YYYY-MM-DD HH24:MI:SS&#x27;) where amount&lt;&gt;99;INSERT 0 3MogDB=# select * from tmp_accounts; name | amount------+-------- A | 100 B | 100 C | 100(3 rows)MogDB=# insert into accounts select * from tmp_accounts;INSERT 0 3MogDB=# select * from accounts; name | amount------+-------- D | 99 A | 100 B | 100 C | 100(4 rows) Alternatively, if no new data has been added to the table after the erroneous operation, you can use the timecapsule table command to flashback the entire table to the specified timestamp. 1234567891011MogDB=# timecapsule table accounts to TIMESTAMP to_timestamp(&#x27;2024-06-13 18:40:18&#x27;,&#x27;YYYY-MM-DD HH24:MI:SS&#x27;);TimeCapsule TableMogDB=# select * from accounts; name | amount------+-------- D | 99 A | 100 B | 100 C | 100(4 rows) Recovering from Truncate or Drop TableIf you accidentally issue a TRUNCATE or DROP command, In this situation, the before commands don’t help because the data has been truncated. 12345678910111213MogDB=# truncate table accounts;TRUNCATE TABLEMogDB=# select * from accounts timecapsule TIMESTAMP to_timestamp(&#x27;2024-06-14 02:10:28&#x27;,&#x27;YYYY-MM-DD HH24:MI:SS&#x27;);ERROR: Snapshot too old, ScanRelation, the info: snapxmax is 45865, snapxmin is 45865, csn is 30047, relfrozenxid64 is 45877, globalRecycleXid is 17356.MogDB=# timecapsule table accounts to TIMESTAMP to_timestamp(&#x27;2024-06-14 02:10:28&#x27;,&#x27;YYYY-MM-DD HH24:MI:SS&#x27;);TimeCapsule TableMogDB=# select * from accounts; name | amount------+--------(0 rows) For TRUNCATE or DROP TABLE operations, use the to before keyword to recover the table from the recycle bin. 1234567891011MogDB=# timecapsule table accounts to before truncate;TimeCapsule TableMogDB=# select * from accounts; name | amount------+-------- A | 100 B | 100 C | 100 D | 99(4 rows) Similarly, if the table was dropped, you can recover it from the recycle bin using the same to before keyword. 12345678910111213141516MogDB=# drop table accounts;DROP TABLEMogDB=# select * from accounts;ERROR: relation &quot;accounts&quot; does not exist on dn_6001LINE 1: select * from accounts; ^MogDB=# timecapsule table accounts to before drop;TimeCapsule TableMogDB=# select * from accounts; name | amount------+-------- A | 100 B | 100 C | 100 D | 99(4 rows) ConclusionMogDB’s flashback feature is an essential tool for recovering from accidental data deletions, truncations, or drops. By enabling the ustore storage engine, setting an appropriate undo_retention_time, and activating the recycle bin, you can leverage flashback queries and the timecapsule command to restore your data efficiently. These features ensure that you can maintain data integrity and quickly recover from human errors, providing robust data protection and operational resilience.","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"}]},{"title":"Exploring Oracle Compatibility in MogDB (Series I) - Data Dictionary","slug":"Exploring-Oracle-Compatibility-in-MogDB-Series-I-Data-Dictionary","date":"2024-06-06T03:45:38.000Z","updated":"2024-06-10T05:32:42.662Z","comments":true,"path":"2024/06/06/Exploring-Oracle-Compatibility-in-MogDB-Series-I-Data-Dictionary/","permalink":"http://kamusis.github.io/2024/06/06/Exploring-Oracle-Compatibility-in-MogDB-Series-I-Data-Dictionary/","excerpt":"","text":"Install PTKPTK is the optimal tool for installing the MogDB database (an enterprise-grade commercial distribution based on openGauss) or the openGauss database, offering a seamless and smooth installation experience. To install PTK, simply run the following command: 1curl --proto &#x27;=https&#x27; --tlsv1.2 -sSf https://cdn-mogdb.enmotech.com/ptk/install.sh | sh This command will automatically install PTK in the user’s home directory under $HOME/.ptk. This directory will serve as the working directory for PTK, storing cache files, metadata information, cluster configuration information, backup information, and other related files. Additionally, the installation command will add the path $HOME/.ptk/bin to the PATH environment variable in the corresponding shell profile file, enabling the user to use the ptk command directly after logging into the server. In this tutorial, we installed PTK using the root user, although this is not mandatory. For more detailed PTK installation instructions, please refer to: PTK Installation Guide. The environment used for this series of articles is CentOS 7.6 for x86-64. MogDB must currently run on a Linux operating system. If you wish to install MogDB on macOS or Windows, you can do so using container deployment. For more information, refer to: Container-based MogDB Installation To check your CentOS version, you can use the following command: 12# cat /etc/centos-releaseCentOS Linux release 7.6.1810 (Core) MogDB can also run on ARM architecture CPUs. You can list all supported CPU brands using the ptk candidate cpu command: 12345678910# ptk candidate cpu CPU Model------------------ Cortex-ARM64 Kunpeng-ARM64 Phythium-ARM64 Hygon-x64 Intel-x64 AMD-x64 zhaoxin-x64 If PTK notifies you of a new version while running any command, you can upgrade it directly using the ptk self upgrade command. 12345678910111213141516171819# ptk ls cluster_name | id | addr | user | data_dir | db_version | create_time | comment---------------+----+------+------+----------+------------+-------------+----------Warning: New version &#x27;1.5.0&#x27; is available, you are using ptk version &#x27;1.1.3&#x27;.You can upgrade ptk via command: &#x27;ptk self upgrade&#x27;You can also set environment by &#x27;export PTK_NO_CHECK_VERSION=true&#x27; to disable this warning# ptk self upgradeINFO[2024-06-06T11:59:01.105] current version: 1.1.3 release, target version: latestINFO[2024-06-06T11:59:01.105] download package from http://cdn-mogdb.enmotech.com/ptk/latest/ptk_linux_x86_64.tar.gzINFO[2024-06-06T11:59:01.105] downloading ptk_linux_x86_64.tar.gz ...&gt; ptk_linux_x86_64.tar.gz: 17.78 MiB / 20.08 MiB [-----------------------------------------------------------------------&gt;_________] 88.52% 27.93 MiB p/s ETA 0s&gt; ptk_linux_x86_64.tar.gz: 20.08 MiB / 20.08 MiB [---------------------------------------------------------------------------------] 100.00% 29.07 MiB p/s 900msINFO[2024-06-06T11:59:02.956] upgrade ptk successfully[root@mogdb-kernel-0004 ~]# ptk versionPTK Version: v1.5.0 releaseGo Version: go1.19.10Build Date: 2024-06-04T17:16:07Git Hash: edd5dbb0OS/Arch: linux/amd64 Install MogDBWith PTK, you can easily create multiple MogDB instances on a single server. Each MogDB instance can be assigned to a different operating system user, allowing you to manage multiple database instances with a single PTK installation. To quickly create a test database without any configuration, you can use the ptk demo command: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ptk demoCluster Name: &quot;demo_kVP&quot;+--------------+--------------+------------------------------+-------+---------+---------------------------------+----------+| az(priority) | ip | user(group) | port | role | data dir | upstream |+--------------+--------------+------------------------------+-------+---------+---------------------------------+----------+| AZ1(1) | 172.16.0.164 | demo_user_kVP(demo_user_kVP) | 26000 | primary | /home/demo_user_kVP/demodb/data | - |+--------------+--------------+------------------------------+-------+---------+---------------------------------+----------+✔ Is cluster config correct (default=n) [y/n]: yINFO[2024-06-06T12:12:05.948] start check hostname ...INFO[2024-06-06T12:12:05.951] check hostname successINFO[2024-06-06T12:12:05.951] skip check osINFO[2024-06-06T12:12:05.951] start check distro ...INFO[2024-06-06T12:12:05.972] check distro success[....]Demo Summary:Deploy Status: cluste_name | host | user | port | status | message--------------+--------------+---------------+-------+---------------+---------- demo_kVP | 172.16.0.164 | demo_user_kVP | 26000 | start_success | successDatabase Detail: item | value----------------+---------------------------------- user_password | Demo&amp;kVP db_password | Demo&amp;kVP base_dir | /home/demo_user_kVP/demodb app_dir | /home/demo_user_kVP/demodb/app data_dir | /home/demo_user_kVP/demodb/data tool_dir | /home/demo_user_kVP/demodb/tool tmp_dir | /home/demo_user_kVP/demodb/tmpMode Compatibility: mode | database | plugins | other-------+------------------------+-----------------------+------------------------------------- PG | postgres_compatibility | none | A | oracle_compatibility | whale[success] | | | compat_tools[success] | | | mogila[success] | db user: mogdb, password: Demo&amp;kVPPlugin Install Details: mode | plugin | status | reference | error-------+--------------+---------+-------------------------------------------+-------- A | whale | success | https://docs.mogdb.io/zh/mogdb/v3.0/whale | | compat_tools | success | https://gitee.com/enmotech/compat-tools | | mogila | success | https://gitee.com/enmotech/mogila | The ptk demo command accomplishes the following tasks: Creates an operating system user named demo_user_kVP. Creates a single-instance MogDB database of the latest version (currently 5.0.7). Sets the database to listen on port 26000. Before running the ptk demo command, ensure that port 26000 is not occupied by other applications. Create an initial database user with the same name as the OS user: demo_user_kVP. Also, create a monitor user named mogdb. Both users will have their initial passwords specified in the “Database Detail” section. Creates two test databases: oracle_compatibility in Oracle-compatible mode and postgres_compatibility in PostgreSQL-compatible mode. For the automatically created oracle_compatibility database, the following enhancements are also performed: Installs the mogila dataset. For more information about this dataset, refer to: Using Sample Dataset Mogila Installs the whale plugin, which provides extensive Oracle compatibility features. For more information on the Whale plugin, refer to: whale Installs the compat-tools utility, which supplements the whale plugin with additional Oracle compatibility features that are not yet implemented, such as Oracle-compatible data dictionary views. For more information on compat-tools, refer to: enmotech&#x2F;compat-tools Note: compat-tools only work within a single database. This means that if you install compat-tools in the oracle_compatibility database (as is done automatically by the ptk demo command), you will only be able to query the corresponding Oracle-compatible data dictionary views when logged into that database. If you want to use these views in another database, you must install compat-tools in that database as well. For example, if you want to query Oracle-compatible data dictionary views in the postgres database, you need to download compat-tools separately from enmotech&#x2F;compat-tools then run the following command: 1gsql -h 127.0.0.1 -p 26000 -d postgres -f runMe.sql Exploring Oracle-Compatible Data Dictionary in MogDBSwitch to the operating system user associated with the demo database. You can log in to the database using the gsql command. Note: If you want to connect to the database remotely, you should use the mogdb user. The ptk demo command creates two users in the database. One starts with demo_user_, which is the initial user. Due to MogDB’s enhanced security features, the initial user is not allowed to connect remotely. The other user is mogdb, which can be used for remote connections. The initial password for the users is displayed at the end of the ptk demo command output. Additionally, you should modify the pg_hba.conf file to allow remote connections. For more information, refer to: Connecting to a Database Remotely. 123456789101112131415161718192021222324252627282930# ptk ls cluster_name | id | addr | user | data_dir | db_version | create_time | comment---------------+------+--------------------+---------------+---------------------------------+------------------------------+---------------------+---------- demo_kVP | 6001 | 172.16.0.164:26000 | demo_user_kVP | /home/demo_user_kVP/demodb/data | MogDB 5.0.7 (build c4707384) | 2024-06-06T12:12:24 |# su - demo_user_kVP$ gsql -d oracle_compatibility -rgsql ((MogDB 5.0.7 build c4707384) compiled at 2024-05-24 10:51:53 commit 0 last mr 1804 )Non-SSL connection (SSL connection is recommended when requiring high-security)Type &quot;help&quot; for help.oracle_compatibility=# \\dt List of relations Schema | Name | Type | Owner | Storage--------+---------------+-------+-------+---------------------------------- public | actor | table | mogdb | &#123;orientation=row,compression=no&#125; public | address | table | mogdb | &#123;orientation=row,compression=no&#125; public | category | table | mogdb | &#123;orientation=row,compression=no&#125; public | city | table | mogdb | &#123;orientation=row,compression=no&#125; public | country | table | mogdb | &#123;orientation=row,compression=no&#125; public | customer | table | mogdb | &#123;orientation=row,compression=no&#125; public | film | table | mogdb | &#123;orientation=row,compression=no&#125; public | film_actor | table | mogdb | &#123;orientation=row,compression=no&#125; public | film_category | table | mogdb | &#123;orientation=row,compression=no&#125; public | inventory | table | mogdb | &#123;orientation=row,compression=no&#125; public | language | table | mogdb | &#123;orientation=row,compression=no&#125; public | payment | table | mogdb | &#123;orientation=row,compression=no&#125; public | rental | table | mogdb | &#123;orientation=row,compression=no&#125; public | staff | table | mogdb | &#123;orientation=row,compression=no&#125; public | store | table | mogdb | &#123;orientation=row,compression=no&#125;(15 rows) All these tables come from the Mogila test dataset. Compat-tools brings a large number of Oracle-compatible data dictionary views (refer to: Oracle-Compatible Views). Here are some simple examples. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566-- sysdate function and dual table is supportedoracle_compatibility=# select sysdate from dual; sysdate--------------------- 2024-06-06 12:47:57(1 row)-- V$ views are supportedoracle_compatibility=# select sid,username,status,program from v$session; sid | username | status | program----------------+---------------+--------+------------------------ 47724487509760 | demo_user_kVP | active | gsql 47724085905152 | demo_user_kVP | idle | WLMArbiter 47724057593600 | demo_user_kVP | idle | WorkloadMonitor 47724032427776 | demo_user_kVP | active | workload 47723769759488 | demo_user_kVP | active | Asp 47723818518272 | demo_user_kVP | idle | statement flush thread 47723696359168 | demo_user_kVP | idle | CfsShrinker 47723730437888 | demo_user_kVP | idle | WDRSnapshot 47723597268736 | demo_user_kVP | idle | ApplyLauncher 47723658610432 | demo_user_kVP | idle | TxnSnapCapturer 47723563190016 | demo_user_kVP | active | JobScheduler(11 rows)-- DBA_ views are supportedoracle_compatibility=# select * from dba_users; username | user_id | password | account_status | lock_date | expiry_date | profile---------------+---------+----------+----------------+-----------+-------------+-------------- demo_user_kVP | 10 | ******** | NORMAL | | | DEFAULT_POOL MOGDB | 18720 | ******** | NORMAL | | | DEFAULT_POOL(2 rows)-- USER_ views are supportedoracle_compatibility=# select table_name,num_rows,last_analyzed from user_tables; table_name | num_rows | last_analyzed---------------+----------+------------------------------- COUNTRY | 0 | CITY | 600 | 2024-06-06 12:13:24.504009+08 ACTOR | 0 | FILM_ACTOR | 5462 | 2024-06-06 12:13:24.518718+08 CATEGORY | 0 | FILM_CATEGORY | 1000 | 2024-06-06 12:13:24.531421+08 LANGUAGE | 0 | FILM | 1000 | 2024-06-06 12:13:24.56158+08 PAYMENT | 0 | CUSTOMER | 0 | INVENTORY | 0 | RENTAL | 0 | STAFF | 0 | ADDRESS | 0 | STORE | 0 |(15 rows)oracle_compatibility=# select count(*) from CITY; count------- 600(1 row)-- some short name views are supported, like IND, OBJ, COLS oracle_compatibility=# select index_name,table_owner,uniqueness,distinct_keys,partitioned from ind where table_name=&#x27;CITY&#x27;; index_name | table_owner | uniqueness | distinct_keys | partitioned-------------------+-------------+------------+---------------+------------- CITY_PKEY | PUBLIC | UNIQUE | 600 | NO IDX_FK_COUNTRY_ID | PUBLIC | NONUNIQUE | 109 | NO(2 rows) Note: Due to significant differences in the underlying structure between MogDB and Oracle, these Oracle-compatible views may not include all the fields present in the corresponding Oracle database views. To be continued: - Exploring Oracle Compatibility in MogDB (Series II) - System Functions - Exploring Oracle Compatibility in MogDB (Series III) - DBMS Packages","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"},{"name":"Oracle-Compatibility","slug":"Oracle-Compatibility","permalink":"http://kamusis.github.io/tags/Oracle-Compatibility/"}]},{"title":"Working Mechanism of synchronous_standby_names in MogDB","slug":"Working-Mechanism-of-synchronous-standby-names-in-MogDB","date":"2024-06-05T02:05:50.000Z","updated":"2024-06-05T02:09:43.089Z","comments":true,"path":"2024/06/05/Working-Mechanism-of-synchronous-standby-names-in-MogDB/","permalink":"http://kamusis.github.io/2024/06/05/Working-Mechanism-of-synchronous-standby-names-in-MogDB/","excerpt":"","text":"In MogDB, the parameter synchronous_standby_names is used to configure synchronous replication settings. When this parameter is set to &#39;*&#39;（default setting）, it indicates that any available standby server can be used as a synchronous standby. This configuration allows any currently connected standby to be utilized as a synchronous standby without explicitly specifying the standby’s name. Working MechanismWhen synchronous_standby_names is set to &#39;*&#39;, MogDB’s synchronous replication mechanism selects the synchronous standby through the following steps: Initial Connection: When the primary server starts or the parameter is changed, the primary server accepts all connected standby servers. Synchronous Standby Confirmation: The primary server sends a synchronization request to all connected standby servers. Each standby server, upon receiving the request, confirms it and reports its status back to the primary server. Selecting the Synchronous Standby: The primary server selects the earliest responding standby server(s) as the synchronous standby. This selection process is dynamic, meaning that if new standbys connect or current synchronous standbys disconnect, the primary server will reselect the synchronous standby. Priority and Behavior No Priority Order: Since &#39;*&#39; denotes any standby, all standbys have the same priority. The primary server simply selects the earliest responding standby as the synchronous standby. Dynamic Adjustment: If a synchronous standby disconnects, the primary server automatically selects the next responding standby as the new synchronous standby, ensuring the continuity and reliability of synchronous replication. Concurrent Management: If multiple standbys connect simultaneously, the primary server can handle these concurrent connections and select the synchronous standby based on the confirmation of synchronization requests. Configuration Example and UsageSuppose we have one primary server and three standby servers (standby1, standby2, and standby3). In the configuration file postgresql.conf, set synchronous_standby_names to &#39;*&#39;: 1synchronous_standby_names = &#x27;*&#x27; Scenario Analysis Selection at Startup: When the primary server starts, all connected standby servers send heartbeat signals and wait for the primary server’s synchronization request. The primary server selects the earliest responding standby as the synchronous standby. Runtime Changes: If the current synchronous standby standby1 disconnects, the primary server automatically selects the next responding standby standby2 as the new synchronous standby. If a new standby standby3 connects to the primary server and there is no current synchronous standby, the primary server selects standby3 as the synchronous standby. Dynamic AdjustmentIf we start the primary server and three standby servers, setting synchronous_standby_names = &#39;*&#39;, the following state transitions are possible: Initial State: All standbys (standby1, standby2, and standby3) connect, and the primary server selects the earliest responding standby standby1 as the synchronous standby. standby1 Disconnects: The primary server automatically selects the next responding standby standby2 as the new synchronous standby. New Standby Connection: A new standby standby4 connects; the primary server will not change the current synchronous standby unless standby2 disconnects. SummaryWhen synchronous_standby_names is set to &#39;*&#39;, MogDB dynamically selects any currently available standby as the synchronous standby. This provides a flexible and highly available synchronous replication mechanism without requiring administrators to manually specify the standby names. The selection process is based on standby response times and automatically adjusts during runtime to ensure the continuity and reliability of synchronous replication.","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"}]},{"title":"MTK: The Ultimate Tool for Seamlessly Migrating Oracle Databases to MogDB","slug":"MTK-The-Ultimate-Tool-for-Seamlessly-Migrating-Oracle-Databases-to-MogDB","date":"2024-06-03T03:57:19.000Z","updated":"2024-06-03T03:58:00.096Z","comments":true,"path":"2024/06/03/MTK-The-Ultimate-Tool-for-Seamlessly-Migrating-Oracle-Databases-to-MogDB/","permalink":"http://kamusis.github.io/2024/06/03/MTK-The-Ultimate-Tool-for-Seamlessly-Migrating-Oracle-Databases-to-MogDB/","excerpt":"","text":"Get the latest version MTK.12wget https://cdn-mogdb.enmotech.com/mtk/v2.6.3/mtk_2.6.3_linux_amd64.tar.gztar -xvf mtk_2.6.3_linux_amd64.tar.gz Generate MTK trial license onlineThe trial license is valid for 1 month, and each email address can generate only one license. However, email addresses with the “enmotech.com” domain can generate licenses repeatedly. If clients wish to extend their trial of MTK beyond the initial month, they should contact Enmotech’s sales or pre-sales team to request an additional 1-month license. 1234567[kamus@altlinux10 mtk_2.6.3_linux_amd64]$ ./mtk license genLicense File Not Found (default license.json)The License code is invalid, start applying✔ Email: kamus@enmotech.com█Start applying for email kamus@enmotech.com authorization.Start parsing the interface to return data.Successful application for authorization. Please check the mail and save it as license.json. Upon receiving the email, upload the attached license.json file to the MTK directory. Then, use the command mtk -v to verify the license validation. 1234567891011121314151617181920212223242526272829[kamus@altlinux10 mtk_2.6.3_linux_amd64]$ ./mtk -vUsing license file: /home/kamus/mogdb-tools/mtk_2.6.3_linux_amd64/license.jsonName: kamus@enmotech.comExpiry: 2022-10-24 12:08:58.751194162 +0800 +0800License key verified!License checks OK!MMMMMMMM MMMMMMMMTTTTTTTTTTTTTTTTTTTTTTTKKKKKKKKK KKKKKKKM:::::::M M:::::::MT:::::::::::::::::::::TK:::::::K K:::::KM::::::::M M::::::::MT:::::::::::::::::::::TK:::::::K K:::::KM:::::::::M M:::::::::MT:::::TT:::::::TT:::::TK:::::::K K::::::KM::::::::::M M::::::::::MTTTTTT T:::::T TTTTTTKK::::::K K:::::KKKM:::::::::::M M:::::::::::M T:::::T K:::::K K:::::KM:::::::M::::M M::::M:::::::M T:::::T K::::::K:::::KM::::::M M::::M M::::M M::::::M T:::::T K:::::::::::KM::::::M M::::M::::M M::::::M T:::::T K:::::::::::KM::::::M M:::::::M M::::::M T:::::T K::::::K:::::KM::::::M M:::::M M::::::M T:::::T K:::::K K:::::KM::::::M MMMMM M::::::M T:::::T KK::::::K K:::::KKKM::::::M M::::::M TT:::::::TT K:::::::K K::::::KM::::::M M::::::M T:::::::::T K:::::::K K:::::KM::::::M M::::::M T:::::::::T K:::::::K K:::::KMMMMMMMM MMMMMMMM TTTTTTTTTTT KKKKKKKKK KKKKKKKRelease version: v2.6.3Git Commit hash: da0ed8eeGit Commit Date: 2022-09-22T01:17:49ZGit Tag : v2.6.3Build timestamp: 20220922011907 Install Oracle instant clientMTK requires the Oracle Instant Client to migrate Oracle objects to MogDB. In this tutorial, we will download the Oracle Instant Client for Linux x86-64 Basic Package. After downloading, unzip the file and set the LD_LIBRARY_PATH parameter as follows: 1export LD_LIBRARY_PATH=/home/kamus/instantclient_21_7:$LD_LIBRARY_PATH In this tutorial, we will migrate a sample schema “customer_orders” to MogDB. The scripts for Oracle can be downloaded from the db-sample-schemas repository on GitHub. Initialize migration project1./mtk init-project -s oracle -t mogdb -n ora2mogdb Modify MTK configuration fileModify the example MTK configuration file stored in the project_name_dir/config directory. Refer to the MTK documentation for detailed information on each parameter. The essential configuration sections for MTK are source, target, and object. source section: This defines the connection to the source database. MTK needs to query the Oracle database dictionary to retrieve DDL. Therefore, it is typically recommended to use a DBA user, with the default system user being sufficient. target section: This defines the connection to the target database. object section: To migrate all objects within a schema, simply specify the schema name in the schemas section. The mtk.json configuration file should look like this: 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;source&quot;: &#123; &quot;type&quot;: &quot;oracle&quot;, &quot;connect&quot;: &#123; &quot;version&quot;: &quot;&quot;, &quot;host&quot;: &quot;119.3.182.31&quot;, &quot;user&quot;: &quot;system&quot;, &quot;port&quot;: 15221, &quot;password&quot;: &quot;oracle&quot;, &quot;dbName&quot;: &quot;ORACLE21C&quot;, &quot;charset&quot;: &quot;&quot;, &quot;clientCharset&quot;: &quot;&quot; &#125; &#125;, &quot;target&quot;: &#123; &quot;type&quot;: &quot;mogdb&quot;, &quot;connect&quot;: &#123; &quot;version&quot;: &quot;&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;user&quot;: &quot;co&quot;, &quot;port&quot;: 26000, &quot;password&quot;: &quot;Enmo@123&quot;, &quot;dbName&quot;: &quot;postgres&quot;, &quot;charset&quot;: &quot;&quot;, &quot;clientCharset&quot;: &quot;&quot; &#125; &#125;, &quot;object&quot;: &#123; &quot;tables&quot;: [], &quot;schemas&quot;: [&quot;co&quot;], &quot;excludeTable&quot;: &#123;&#125;, &quot;tableSplit&quot;: &#123;&#125; &#125;, &quot;dataOnly&quot;: false, &quot;schemaOnly&quot;: false&#125; For this tutorial, we plan to migrate all objects in the “CO” schema from the Oracle database to the same user in MogDB. For testing purposes, we will not create a new database in MogDB. Instead, we will create a new user “co” in the default database, postgres. Run the following command to create the user: 1234567[omm@altlinux10 ~]$ gsql -d postgres -p 26000 -rgsql ((MogDB 3.0.2 build 9bc79be5) compiled at 2022-09-18 00:37:49 commit 0 last mr )Non-SSL connection (SSL connection is recommended when requiring high-security)Type &quot;help&quot; for help.MogDB=# create user co identified by &quot;Enmo@123&quot;;CREATE ROLE Start migrationNow, we can start migration. 1./mtk -c ora2mogdb/config/mtk.json Check migration reportThe migration result report will be generated in the project report directory, available in both plain text and HTML formats. For simplicity, I have included the text format result in this tutorial. 123456789101112131415161718192021222324252627-----------------------ObjectName Type Summary-----------------------+------------------+-------------------+-------------------+--------+-----------+-------------+-------------+-------------+--------------------|-------------+| Type | StartTime | EndTime | Status | Total Num | Success Num | Warring Num | Failed Num |Failed(Invalid) Num | Time |+------------------+-------------------+-------------------+--------+-----------+-------------+-------------+-------------+--------------------|-------------+|Schema |2022-09-24 15:12:36|2022-09-24 15:12:36|finish |1 |1 |0 |0 |0 |282 ms ｜|Sequence |2022-09-24 15:12:36|2022-09-24 15:12:36|finish |0 |0 |0 |0 |0 |210 ms ｜|ObjectType |2022-09-24 15:12:36|2022-09-24 15:12:36|finish |0 |0 |0 |0 |0 |356 ms ｜|Queue |2022-09-24 15:12:36|2022-09-24 15:12:37|finish |0 |0 |0 |0 |0 |177 ms ｜|Table |2022-09-24 15:12:37|2022-09-24 15:12:47|finish |7 |7 |0 |0 |0 |9 s 952 ms ｜|TableData |2022-09-24 15:12:47|2022-09-24 15:12:53|finish |7 |7 |0 |0 |0 |6 s 743 ms ｜|Index |2022-09-24 15:12:53|2022-09-24 15:12:53|finish |7 |7 |0 |0 |0 |1 ms ｜|Constraint |2022-09-24 15:12:53|2022-09-24 15:12:53|finish |24 |23 |0 |1 |0 |51 ms ｜|DBLink |2022-09-24 15:12:53|2022-09-24 15:12:53|finish |0 |0 |0 |0 |0 |67 ms ｜|View |2022-09-24 15:12:53|2022-09-24 15:12:54|finish |4 |2 |0 |2 |0 |723 ms ｜|MaterializedView |2022-09-24 15:12:54|2022-09-24 15:12:54|finish |0 |0 |0 |0 |0 |138 ms ｜|Function |2022-09-24 15:12:54|2022-09-24 15:12:54|finish |0 |0 |0 |0 |0 |113 ms ｜|Procedure |2022-09-24 15:12:54|2022-09-24 15:12:55|finish |0 |0 |0 |0 |0 |109 ms ｜|Package |2022-09-24 15:12:55|2022-09-24 15:12:55|finish |0 |0 |0 |0 |0 |77 ms ｜|Trigger |2022-09-24 15:12:55|2022-09-24 15:12:55|finish |0 |0 |0 |0 |0 |404 ms ｜|Synonym |2022-09-24 15:12:55|2022-09-24 15:12:55|finish |0 |0 |0 |0 |0 |74 ms ｜|TableDataCom |2022-09-24 15:12:55|2022-09-24 15:12:56|finish |7 |7 |0 |0 |0 |810 ms ｜|AlterSequence |2022-09-24 15:12:56|2022-09-24 15:12:56|finish |0 |0 |0 |0 |0 |71 ms ｜|CollStatistics |2022-09-24 15:12:56|2022-09-24 15:12:56|finish |7 |7 |0 |0 |0 |29 ms ｜+------------------+-------------------+-------------------+--------+-----------+-------------+-------------+-------------+--------------------|-------------+ We can see that all tables and table data have been successfully migrated to MogDB without any errors. However, there is one failed constraint and two failed views. The failed constraint is a JSON check constraint, which is not supported by MogDB. The failed views are due to the grouping_id function and the json_table function, which are not yet implemented in MogDB (). SQL ERROR pq: function grouping_id(character varying, character varying) does not exist pq: syntax error at or near “columns” Check migration resultRun the sample queries to ensure that all the data has been migrated to MogDB without any errors. 1234567891011121314/* 5 products with the highest revenue With their corresponding order rank */select p.product_name, count(*) number_of_orders, sum ( oi.quantity * oi.unit_price ) total_value, rank () over ( order by count(*) desc ) order_count_rankfrom products pjoin order_items oion p.product_id = oi.product_idgroup by p.product_nameorder by sum ( oi.quantity * oi.unit_price ) descfetch first 5 rows only; 1234567891011121314151617181920MogDB=&gt; select p.product_name,MogDB-&gt; count(*) number_of_orders,MogDB-&gt; sum ( oi.quantity * oi.unit_price ) total_value,MogDB-&gt; rank () over (MogDB(&gt; order by sum ( oi.quantity * oi.unit_price ) descMogDB(&gt; ) revenue_rankMogDB-&gt; from products pMogDB-&gt; join order_items oiMogDB-&gt; on p.product_id = oi.product_idMogDB-&gt; group by p.product_nameMogDB-&gt; order by count(*) descMogDB-&gt; fetch first 5 rows only; product_name | number_of_orders | total_value | revenue_rank-----------------------+------------------+-------------+-------------- Girl&#x27;s Trousers (Red) | 148 | 15794.76 | 1 Boy&#x27;s Hoodie (Grey) | 100 | 3754.08 | 35 Men&#x27;s Pyjamas (Blue) | 100 | 3274.61 | 36 Men&#x27;s Coat (Red) | 98 | 4230.30 | 31 Boy&#x27;s Socks (White) | 98 | 3081.12 | 38(5 rows) ConclusionMigrating tables, table data, and indexes from Oracle to MogDB typically proceeds without issues. However, for views, procedures, functions, and packages, some modifications to the source code may be necessary. With the ongoing development of Oracle compatibility in MogDB, we believe that the need for such modifications will decrease over time, making the migration process from Oracle to MogDB even smoother.","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"},{"name":"Oracle","slug":"Oracle","permalink":"http://kamusis.github.io/tags/Oracle/"},{"name":"Database Migration","slug":"Database-Migration","permalink":"http://kamusis.github.io/tags/Database-Migration/"}]},{"title":"Ensuring Data Integrity: How Major Databases Handle Partial Writes and Atomic Operations","slug":"Ensuring-Data-Integrity-How-Major-Databases-Handle-Partial-Writes-and-Atomic-Operations","date":"2024-05-30T10:22:37.000Z","updated":"2024-05-31T16:21:32.132Z","comments":true,"path":"2024/05/30/Ensuring-Data-Integrity-How-Major-Databases-Handle-Partial-Writes-and-Atomic-Operations/","permalink":"http://kamusis.github.io/2024/05/30/Ensuring-Data-Integrity-How-Major-Databases-Handle-Partial-Writes-and-Atomic-Operations/","excerpt":"","text":"In the world of databases, ensuring atomicity in write operations is crucial. This post explores the importance of atomic writes and how various databases handle potential issues like partial writes. Understanding Partial WritesIn the I&#x2F;O write chain, the size of atomic operations can vary. An operation at a higher level may be split into multiple operations at a lower level, leading to scenarios where some operations succeed while others fail. This phenomenon is known as a partial write. Legacy Disk Drives: Default sector size is 512 bytes. Modern Disk Drives: Advanced format sector size is 4k. Filesystem Page Size: For example, ext4 has a default page size of 4k. Database Page Sizes: Oracle: 8K MySQL: 16K PostgreSQL: 8K openGauss: 8K The Reality of Partial WritesWhile theoretically, databases must handle partial writes, in practice, such events are quite rare. The critical aspect is not just preventing partial writes but ensuring robust mechanisms for data integrity checks (checksums) and recovery when issues do arise. How Oracle Handles Partial WritesOracle’s approach to partial writes is pragmatic: Automatic Repair: If a fractured block is detected, Oracle tries to repair it using the corresponding block from Active Data Guard (ADG) or the flashback log. Error Reporting: If automatic repair fails, an ORA-01578 error is logged, prompting the DBA to perform a media recovery. Block-Level Recovery: Oracle supports block-level recovery, making the process less cumbersome and time-consuming. Oracle’s backup sets are designed to handle partial writes: Using OS Commands: When backing up data files using OS commands, Oracle requires the tablespace to be in backup mode. Any changes to data blocks are logged in the redo log, ensuring a clean block can be restored during recovery. Using RMAN: RMAN handles partial writes by ensuring that each backup block is clean, eliminating the need to worry about fractured blocks. Block Checking: Oracle provides mechanisms like DB_BLOCK_CHECKING for comprehensive checksums. How MySQL Handles Partial WritesMySQL employs a double write buffer to safeguard against partial writes. Double Write Buffer: Despite its name, the double write buffer is not stored in memory; instead, it consists of persistent files on disk. The mechanism works as follows: before writing a data block to its final location on disk, MySQL first writes it to the double write buffer. Data Integrity: This approach ensures that there is always a clean copy of the block available—either in the double write buffer or in the data files—allowing for safe redo operations. However, this method is not without its challenges. One of the primary concerns is its efficiency, as it effectively doubles the I&#x2F;O workload for redo logging. This has sparked ongoing discussions about the trade-offs and effectiveness of using a double write buffer to address partial writes. How PostgreSQL Handles Partial WritesPostgreSQL employs full page writes to manage partial writes effectively: Full Page Writes: When a data block is updated for the first time after a checkpoint, the entire block is written to the WAL (Write-Ahead Logging) log. This is akin to the database always being in “Backup status,” similar to issuing the alter tablespace x begin backup command in an Oracle database. Data Integrity: This mechanism ensures that even if a partial write occurs, the database remains consistent because the original page can be used for redo operations. There has been community discussion about adopting a double write buffer mechanism, but it remains an area of interest rather than active development. For more details, you can check out these discussions: Discussion on Double Write Buffer Mechanism (Post 1) Discussion on Double Write Buffer Mechanism (Post 2) How MogDB Handles Partial WritesMogDB (the enterprise edition of openGauss) combines strategies from MySQL and PostgreSQL: Double Write Buffer: Similar to MySQL, MogDB uses a double write buffer by setting enable_double_write=on. Full Page Writes: MogDB retains PostgreSQL’s full page write mechanism by setting full_page_writes=on. Depending on the configuration of incremental checkpoints (enable_incremental_checkpoint): Incremental Checkpoints Off: With full_page_writes=on, full page writes are used. With full_page_writes=off, partial writes are not a concern. Incremental Checkpoints On: With enable_double_write=on, double writes are used. With enable_double_write=off, the behavior depends on full_page_writes settings. ConclusionEnsuring atomic writes in databases is essential for data integrity and consistency. While partial writes are rare, robust mechanisms for detection and recovery are vital. Oracle, MySQL, PostgreSQL, and MogDB&#x2F;openGauss each have unique approaches to handling these scenarios, ensuring that data remains reliable and recoverable.","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"},{"name":"openGauss","slug":"openGauss","permalink":"http://kamusis.github.io/tags/openGauss/"},{"name":"Oracle","slug":"Oracle","permalink":"http://kamusis.github.io/tags/Oracle/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://kamusis.github.io/tags/PostgreSQL/"},{"name":"MySQL","slug":"MySQL","permalink":"http://kamusis.github.io/tags/MySQL/"},{"name":"Partial Writes","slug":"Partial-Writes","permalink":"http://kamusis.github.io/tags/Partial-Writes/"}]},{"title":"How to build blog site by using Hexo in Windows WSL and deploy to GitHub Pages","slug":"How-to-build-blog-site-by-using-Hexo-in-Windows-WSL-and-deploy-to-GitHub-Pages","date":"2024-05-27T07:10:17.000Z","updated":"2024-05-27T09:50:19.586Z","comments":true,"path":"2024/05/27/How-to-build-blog-site-by-using-Hexo-in-Windows-WSL-and-deploy-to-GitHub-Pages/","permalink":"http://kamusis.github.io/2024/05/27/How-to-build-blog-site-by-using-Hexo-in-Windows-WSL-and-deploy-to-GitHub-Pages/","excerpt":"","text":"Why use WSLUsing Windows Subsystem for Linux (WSL) instead of relying solely on Windows can offer several advantages, particularly for developers and IT professionals. WSL allows users to run a GNU&#x2F;Linux environment directly on Windows, without the overhead of a traditional virtual machine or dual-boot setup. This enables seamless access to a wide range of Linux tools and utilities, which are often preferred for development tasks, scripting, and system administration. Additionally, WSL provides a more consistent and familiar environment for those accustomed to Unix-based systems, facilitating smoother workflows and integration with cloud-based services. By leveraging WSL, professionals can enjoy the best of both worlds: the robust software ecosystem of Windows and the powerful command-line capabilities of Linux. Why use HexoHexo offers several advantages over WordPress, particularly for developers and tech-savvy users. As a static site generator, Hexo provides faster load times and improved security since it does not rely on a database or server-side processing, which are common vulnerabilities in WordPress. Additionally, Hexo allows for greater customization through its use of Markdown and extensive plugin ecosystem, enabling users to tailor their sites to specific needs without the overhead of a complex content management system. Furthermore, Hexo’s deployment process is streamlined, often integrating seamlessly with version control systems like Git, making it an excellent choice for those who prioritize efficiency and performance in their web development projects. Why use Github PagesUsing GitHub Pages instead of a traditional hosting server offers several distinct advantages. Firstly, GitHub Pages provides a seamless integration with GitHub repositories, enabling automatic deployment of websites directly from your codebase. This integration ensures that updates and changes to your site are effortlessly managed through version control, promoting a streamlined workflow. Additionally, GitHub Pages is cost-effective, offering free hosting with custom domain support, which can significantly reduce overhead costs for personal projects or small businesses. The platform also boasts robust security features, leveraging GitHub’s infrastructure to protect your site from common vulnerabilities. Furthermore, the simplicity and ease of use make GitHub Pages an attractive option for developers who want to focus on writing code rather than managing server configurations and maintenance. Overall, GitHub Pages combines efficiency, cost savings, and security, making it an excellent choice for hosting static websites and project documentation. GO!Assume already has a WSL environment runningIf not, refer to “How to install Linux on Windows with WSL“. To say we are running WSL distro - Ubuntu 22.04 12345678910111213$ cat /etc/os-releasePRETTY_NAME=&quot;Ubuntu 22.04.4 LTS&quot;NAME=&quot;Ubuntu&quot;VERSION_ID=&quot;22.04&quot;VERSION=&quot;22.04.4 LTS (Jammy Jellyfish)&quot;VERSION_CODENAME=jammyID=ubuntuID_LIKE=debianHOME_URL=&quot;https://www.ubuntu.com/&quot;SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;UBUNTU_CODENAME=jammy Install Node.jsSince Hexo is written by Node.js, nodejs must be installed. 12sudo apt updatesudo apt install nodejs npm Install GITSince Hexo uses Git to publish pages on GitHub, Git must be installed. 1sudo apt install git Install Hexo1sudo npm install -g hexo-cli Init Hexo projectOur goal is to deploy the static HTML file generated by Hexo to GitHub Pages. To achieve this, we need to create a repository named “[username].github.io” on GitHub. Therefore, we will initialize this directory directly using Hexo. Note: Be sure to replace [username] with your actual GitHub username. 1hexo init kamusis.github.io Create Github pagesCreating GitHub Pages is very simple, reference: https://pages.github.com/ Connecting to Github with SSHReference: https://docs.github.com/en/authentication/connecting-to-github-with-ssh Basically, generate a key using the ssh-keygen command and upload the public key to the GitHub. Once the SSH connection is established, use the following command to verify it: 12$ ssh -T git@github.comHi kamusis! You&#x27;ve successfully authenticated, but GitHub does not provide shell access. Set URL for your new blog123456$ vi _config.yml~~~~~~~~~~~~~~~~~~ _config.yml ~~~~~~~~~~~~~~~~~~# URL## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;url: http://kamusis.github.io Set Git information to let hexo can push contents into Github Pages12345678910$ npm install hexo-deployer-git --save$ vi _config.yml~~~~~~~~~~~~~~~~~~ _config.yml ~~~~~~~~~~~~~~~~~~# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: git repo: git@github.com:kamusis/kamusis.github.io.git branch: master Generate the site and push to Github pages12$ hexo clean$ hexo deploy Assuming everything has been set up correctly, you can now view a hello-world article on a website by navigating to https:&#x2F;&#x2F;[username].github.io. Write your articles and publish1hexo new &quot;My first post&quot; A markdown file will automatically be created and placed in the source&#x2F;_posts directory. It can then be edited and saved using your preferred text or markdown editor. Of course, you can also clean up the initial hello-world article. 1rm source/_posts/hello-world.md Publish the articles. 12$ hexo clean$ hexo deploy Reference: https://hexo.io/docs/github-pages#One-command-deployment https://gist.github.com/btfak/18938572f5df000ebe06fbd1872e4e39","categories":[],"tags":[{"name":"WSL","slug":"WSL","permalink":"http://kamusis.github.io/tags/WSL/"},{"name":"Hexo","slug":"Hexo","permalink":"http://kamusis.github.io/tags/Hexo/"},{"name":"Github Pages","slug":"Github-Pages","permalink":"http://kamusis.github.io/tags/Github-Pages/"}]},{"title":"How to Find the Corresponding Session in MogDB/openGauss from OS Thread ID","slug":"How-to-Find-the-Corresponding-Session-in-MogDB-openGauss-from-OS-Thread-ID","date":"2024-05-27T06:55:48.000Z","updated":"2024-05-27T07:01:07.248Z","comments":true,"path":"2024/05/27/How-to-Find-the-Corresponding-Session-in-MogDB-openGauss-from-OS-Thread-ID/","permalink":"http://kamusis.github.io/2024/05/27/How-to-Find-the-Corresponding-Session-in-MogDB-openGauss-from-OS-Thread-ID/","excerpt":"","text":"Diagnostic NeedsWhen the MogDB database consumes a significant amount of system resources, such as nearly 100% CPU usage, how can we determine which session(s) in the database are using these resources? In Oracle databases, diagnosing such issues typically involves associating v$session, v$process, and the OS process ID found using the top or ps commands. However, MogDB uses a thread model, and only one process ID is visible at the OS level. So, how do we pinpoint the problem? Since MogDB uses a thread model, unlike PostgreSQL’s process model, checking network port access with the lsof command at the OS level will show multiple user sessions connected, but only the process ID will be displayed in the PID column, not the thread. 12345678910$ lsof -i 4 -a -p `pgrep -u omm3 mogdb`COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmogdb 12027 omm3 8u IPv4 20313752 0t0 TCP *:biimenu (LISTEN)mogdb 12027 omm3 9u IPv4 20313753 0t0 TCP *:18001 (LISTEN)mogdb 12027 omm3 325u IPv4 28320946 0t0 TCP mogdb-kernel-0004:biimenu-&gt;softbank060132034173.bbtec.net:45206 (ESTABLISHED)mogdb 12027 omm3 330u IPv4 28316174 0t0 TCP mogdb-kernel-0004:biimenu-&gt;softbank060132034173.bbtec.net:45208 (ESTABLISHED)mogdb 12027 omm3 336u IPv4 28302815 0t0 TCP mogdb-kernel-0004:biimenu-&gt;softbank060132034173.bbtec.net:45210 (ESTABLISHED)mogdb 12027 omm3 340u IPv4 28323140 0t0 TCP mogdb-kernel-0004:biimenu-&gt;softbank060132034173.bbtec.net:45212 (ESTABLISHED)mogdb 12027 omm3 360u IPv4 28323141 0t0 TCP mogdb-kernel-0004:biimenu-&gt;softbank060132034173.bbtec.net:45214 (ESTABLISHED)mogdb 12027 omm3 375u IPv4 28305050 0t0 TCP mogdb-kernel-0004:biimenu-&gt;softbank060132034173.bbtec.net:45216 (ESTABLISHED) How to Get Thread IDYou can use htop. After opening htop, press F5 to display the process tree. The first PID is the process ID, and each line under the tree structure shows the corresponding thread ID for that process. You can also use the ps command. The -L parameter displays threads, and the -o parameter specifies the columns of interest. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# ps -Lp `pgrep -u omm3 mogdb` -o %cpu,tid,pid,ppid,cmd,comm%CPU TID PID PPID CMD COMMAND 0.0 17847 17847 1 /opt/mogdb3/app/bin/mogdb - mogdb 0.0 17848 17847 1 /opt/mogdb3/app/bin/mogdb - jemalloc_bg_thd 0.0 17854 17847 1 /opt/mogdb3/app/bin/mogdb - mogdb 0.0 17855 17847 1 /opt/mogdb3/app/bin/mogdb - syslogger 0.0 17856 17847 1 /opt/mogdb3/app/bin/mogdb - reaper 0.0 17857 17847 1 /opt/mogdb3/app/bin/mogdb - jemalloc_bg_thd 0.0 17858 17847 1 /opt/mogdb3/app/bin/mogdb - jemalloc_bg_thd 0.0 17860 17847 1 /opt/mogdb3/app/bin/mogdb - jemalloc_bg_thd 0.0 17884 17847 1 /opt/mogdb3/app/bin/mogdb - checkpointer 0.0 17885 17847 1 /opt/mogdb3/app/bin/mogdb - Spbgwriter 0.1 17886 17847 1 /opt/mogdb3/app/bin/mogdb - pagewriter 0.0 17887 17847 1 /opt/mogdb3/app/bin/mogdb - pagewriter 0.0 17888 17847 1 /opt/mogdb3/app/bin/mogdb - pagewriter 0.0 17889 17847 1 /opt/mogdb3/app/bin/mogdb - pagewriter 0.0 17890 17847 1 /opt/mogdb3/app/bin/mogdb - pagewriter 0.8 17891 17847 1 /opt/mogdb3/app/bin/mogdb - WALwriter 0.0 17892 17847 1 /opt/mogdb3/app/bin/mogdb - WALwriteraux 0.0 17893 17847 1 /opt/mogdb3/app/bin/mogdb - AVClauncher 0.0 17894 17847 1 /opt/mogdb3/app/bin/mogdb - Jobscheduler 0.0 17895 17847 1 /opt/mogdb3/app/bin/mogdb - asyncundolaunch 0.0 17896 17847 1 /opt/mogdb3/app/bin/mogdb - globalstats 0.0 17897 17847 1 /opt/mogdb3/app/bin/mogdb - applylauncher 0.0 17898 17847 1 /opt/mogdb3/app/bin/mogdb - statscollector 0.0 17899 17847 1 /opt/mogdb3/app/bin/mogdb - snapshotworker 0.1 17900 17847 1 /opt/mogdb3/app/bin/mogdb - TrackStmtWorker 0.0 17901 17847 1 /opt/mogdb3/app/bin/mogdb - 2pccleaner 0.0 17902 17847 1 /opt/mogdb3/app/bin/mogdb - faultmonitor 0.0 17904 17847 1 /opt/mogdb3/app/bin/mogdb - undorecycler 0.0 18372 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18373 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18374 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18375 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18376 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18377 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18378 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18379 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18380 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18381 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18382 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 18454 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 19475 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 19480 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 29529 17847 1 /opt/mogdb3/app/bin/mogdb - worker 0.0 30999 17847 1 /opt/mogdb3/app/bin/mogdb - worker The rows in the comm column that display as “worker” are backend processes of user sessions in the database. Typically, user sessions that consume high CPU can be filtered to show only user sessions using the grep command. How to Map OS Thread ID to Database SessionSuppose on this server, the thread with ID 18372 is consuming a lot of CPU. In MogDB, you can query the pg_os_threads view to find the session ID corresponding to this thread. 123456MogDB=# select * from pg_os_threads where lwpid=18372;node_name | pid | lwpid | thread_name | creation_time----------+------------------+-------+-------------+------------------------------dn_6001 | 140545137571584 | 18372 | dn_6001 | 2022-05-30 19:54:42.459129+08(1 row) The pg_os_threads view records the relationship between lightweight thread IDs and session IDs. The lwpid column is the OS thread ID, and the pid column is the database session ID. For detailed information, refer to the MogDB documentation on PG_OS_THREADS. If you have monadmin privileges, you can also query the os_threads view in the dbe_perf schema, which provides the same information. After finding the database session ID, you can perform various actions, such as querying the dbe_perf.session_stat_activity view to get the application name, client IP address, and the SQL query being executed by that session. 123456MogDB=# select application_name, client_addr, query from dbe_perf.session_stat_activity where pid=140545137571584;application_name | client_addr | query-----------------+-------------+---------------------------------------------dn_6001 | 172.16.0.176| SELECT cfg_value FROM bmsql_config WHERE cfg_name = $1(1 row) You can also query the dbe_perf.thread_wait_status view to get the current wait event of the session. Note that this view contains the lwtid field, which directly corresponds to the thread ID. 123456MogDB=# select lwtid, wait_status, wait_event from dbe_perf.thread_wait_status where sessionid=140545137571584;lwtid | wait_status | wait_event------+-------------+-----------18372 | wait cmd | wait cmd(1 row)","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"}]},{"title":"MogDB ASH: Unveiling the Active Session History for Powerful Database Performance Tuning","slug":"MogDB-ASH-Unveiling-the-Active-Session-History-for-Powerful-Database-Performance-Tuning","date":"2024-05-27T06:48:34.000Z","updated":"2024-05-27T07:01:32.325Z","comments":true,"path":"2024/05/27/MogDB-ASH-Unveiling-the-Active-Session-History-for-Powerful-Database-Performance-Tuning/","permalink":"http://kamusis.github.io/2024/05/27/MogDB-ASH-Unveiling-the-Active-Session-History-for-Powerful-Database-Performance-Tuning/","excerpt":"","text":"What is ASH?ASH, or Active Session History, is a feature in Oracle databases that records information about active sessions at regular intervals in both the database memory and persistent system tables. Data in memory is cleared after a database restart, but data in the persistent system tables is retained for a longer period. ASH is extremely useful for diagnosing issues when the database encounters problems or performance issues by allowing retrospective analysis to identify the root cause. In MogDB, a similar ASH capability is implemented. ASH Capability in MogDBThe ASH capability in MogDB is divided into two parts: the open-source community version (openGauss) and the enhanced enterprise version. 1. openGauss Community Version:MogDB inherits the ASH capability from openGauss. The database provides two main views: dbe_perf.LOCAL_ACTIVE_SESSION and GS_ASP. The LOCAL_ACTIVE_SESSION view is an in-memory table, while GS_ASP is a persistent table. These views contain sampling information about current active sessions. Key parameters affecting ASH functionality: enable_asp: Set to on or off to enable or disable ASH functionality. Default is on. asp_sample_interval: Specifies the interval between each sample. Default is 1 second. To reduce sampling pressure, this can be set to a longer interval, up to a maximum of 10 seconds. asp_sample_num: Specifies the total number of samples retained in the LOCAL_ACTIVE_SESSION in-memory table. Exceeding this number triggers a flush of in-memory samples to the GS_ASP system table and clears the in-memory table. Default is 100,000 samples. asp_flush_rate: Determines which in-memory samples are flushed to the GS_ASP table. Samples with sampleid % asp_flush_rate == 0 are marked as need_flush_sample=true and are persisted. Default is 10, meaning 1&#x2F;10 of samples are persisted. asp_retention_days: Specifies the retention period for data in the GS_ASP table. Default is 2 days, with a maximum of 7 days. 2. MogDB Enterprise Version:The enterprise version enhances ASH capability, termed “SQL Execution State Observation,” by adding sampling of SQL execution operators. The plan_node_id field is added to views to record the execution operator during each sample, allowing identification of which step in the execution plan is causing performance issues. Key parameters affecting enterprise ASH functionality: resource_track_level: Set to operator to enable operator sampling capability. Default is query, which records only SQL-level sampling. LOCAL_ACTIVE_SESSION ViewThis view records extensive information, including session ID, wait events, and SQL query ID (which can be linked to dbe_perf.statement_history or dbe_perf.statement_complex_runtime to retrieve SQL text and execution plans). Note: The plan_node_id field exists only in the enterprise version. Recording SQL Execution PlansQuerying the execution plans of past or currently executing SQL is a common requirement in database maintenance. In MogDB, the following views record SQL execution plans: dbe_perf.STATEMENT_HISTORY: Records information about completed SQL, including execution plans (query_plan field). dbe_perf.STATEMENT_COMPLEX_RUNTIME: Records information about currently executing SQL, including execution plans (query_plan field). Parameters affecting recording of SQL execution plans: enable_resource_track: Enables or disables resource tracking. Default is on. If set to off, no user SQL execution information, including execution plans, is tracked. resource_track_cost: Sets the minimum execution cost for SQL statements to be tracked. Only SQL with a cost above this value will have its execution plan recorded. Comprehensive Query ExampleThe following query retrieves all information about ongoing SQL executions, including historical samples: 123SELECT las.sample_time, las.application_name, las.unique_query_id, las.event, scr.query, scr.query_planFROM dbe_perf.local_active_session las, dbe_perf.statement_complex_runtime scrWHERE las.thread_id = scr.pid AND scr.pid &lt;&gt; pg_backend_pid(); Example output shows a frequently executed full table scan query, including SQL text, execution plan, and client information, providing valuable data for performance diagnosis.","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"}]},{"title":"How to move WSL distro in Windows 11 to another drive","slug":"How-to-move-WSL-distro-in-Windows-11-to-another-drive","date":"2024-05-27T06:17:24.000Z","updated":"2024-05-27T09:45:08.699Z","comments":true,"path":"2024/05/27/How-to-move-WSL-distro-in-Windows-11-to-another-drive/","permalink":"http://kamusis.github.io/2024/05/27/How-to-move-WSL-distro-in-Windows-11-to-another-drive/","excerpt":"","text":"Introduction:In the world of development and system administration, Windows Subsystem for Linux (WSL) has become a valuable tool. It allows users to run a Linux distribution alongside their Windows environment, opening up a world of possibilities for developers and administrators. In this article, we’ll guide you through the process of migrating a WSL instance, using a real-world example, step by step. Prerequisites:Before we begin, ensure that you have the following prerequisites in place: Windows 10 or later with WSL installed. An existing WSL instance (in our case, Ubuntu). Sufficient storage space for the migration. Step 1: Create a Target DirectoryTo start the migration process, we need a target directory to store the migrated WSL instance. In PowerShell, use the ‘mkdir’ command to create this directory. In our example, we create a directory named ‘D:\\WSL\\Ubuntu’: 1mkdir -p D:\\WSL\\Ubuntu Step 2: List All Running WSL InstancesBefore we proceed further, let’s list all the running WSL instances. The following command will display a list of all WSL instances, including their state and version: 1wsl -l --all -v Step 3: Export the Source WSL InstanceNow, let’s export the source WSL instance (in our case, ‘Ubuntu’) into a tar file. This step automatically shuts down the WSL instance and restarts it after the export: 1wsl --export Ubuntu D:\\WSL\\Ubuntu.tar Step 4: Unregister the Source WSL InstanceOnce the export is complete, we need to unregister the source WSL instance to avoid conflicts. Use the following command: 1wsl --unregister Ubuntu Step 5: Confirm UnregistrationTo confirm that the source WSL instance has been successfully unregistered, run the following command: 1wsl -l --all -v Step 6: Import into the Target DirectoryNow it’s time to import the previously exported WSL instance into the target directory. In this step, we specify the target directory and version (in our case, version 2): 1wsl --import Ubuntu D:\\WSL\\Ubuntu D:\\WSL\\Ubuntu.tar --version 2 Step 7: Verify the MigrationTo ensure that the migration was successful, list all WSL instances once again: 1wsl -l --all -v Step 8: Access the Migrated WSL InstanceNow, you can access the migrated WSL instance by using the following command: 1wsl -d Ubuntu Conclusion:Migrating WSL instances is a powerful way to manage and organize your development environments. By following these steps, you can seamlessly move your WSL instances to different directories or machines, ensuring flexibility and efficiency in your development workflow. Keep in mind that WSL provides a bridge between Windows and Linux, allowing you to enjoy the best of both worlds. Check the all steps screenshot as below. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# create target directoryPS C:\\Users\\kamus&gt; mkdir -p D:\\WSL\\Ubuntu# List all the wsl runningPS C:\\Users\\kamus&gt; wsl -l --all -v NAME STATE VERSION* Ubuntu Running 2 docker-desktop Stopped 2 docker-desktop-data Stopped 2 # Export source wslPS C:\\Users\\kamus&gt; wsl --export Ubuntu D:\\WSL\\Ubuntu.tar# When doing export, wsl will be shutdown automatically and restart after exportingPS C:\\Users\\kamus&gt; wsl -l --all -v NAME STATE VERSION* Ubuntu Running 2 docker-desktop Stopped 2 docker-desktop-data Stopped 2 # Unregister the source wslPS C:\\Users\\kamus&gt; wsl --unregister Ubuntu正在注销...# Check unregister is successfulPS C:\\Users\\kamus&gt; wsl -l --all -v NAME STATE VERSION* docker-desktop Stopped 2 docker-desktop-data Stopped 2 # Import into the target directoryPS C:\\Users\\kamus&gt; wsl --import Ubuntu D:\\WSL\\Ubuntu D:\\WSL\\Ubuntu.tar --version 2# Check resultsPS C:\\Users\\kamus&gt; wsl -l --all -v NAME STATE VERSION* docker-desktop Stopped 2 Ubuntu Stopped 2 docker-desktop-data Stopped 2PS C:\\Users\\kamus&gt; wsl -d UbuntuWelcome to Ubuntu 20.04.5 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Fri Jan 5 14:40:25 JST 2024 System load: 0.68 Processes: 8 Usage of /: 2.0% of 250.98GB Users logged in: 0 Memory usage: 4% IPv4 address for eth0: 172.28.208.11 Swap usage: 0%0 updates can be applied immediately.The list of available updates is more than a week old.To check for new updates run: sudo apt updateThis message is shown once a day. To disable it please create the/root/.hushlogin file.root@Kamus-Trident:/mnt/c/Users/kamus# cat /etc/os-releaseNAME=&quot;Ubuntu&quot;VERSION=&quot;20.04.5 LTS (Focal Fossa)&quot;ID=ubuntuID_LIKE=debianPRETTY_NAME=&quot;Ubuntu 20.04.5 LTS&quot;VERSION_ID=&quot;20.04&quot;HOME_URL=&quot;https://www.ubuntu.com/&quot;SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;VERSION_CODENAME=focalUBUNTU_CODENAME=focalroot@Kamus-Trident:/mnt/c/Users/kamus#","categories":[],"tags":[{"name":"Windows","slug":"Windows","permalink":"http://kamusis.github.io/tags/Windows/"},{"name":"WSL","slug":"WSL","permalink":"http://kamusis.github.io/tags/WSL/"}]},{"title":"Unlocking the Power: Key Features of MogDB","slug":"Unlocking-the-Power-Key-Features-of-MogDB","date":"2024-05-27T06:16:11.000Z","updated":"2024-05-27T06:16:59.322Z","comments":true,"path":"2024/05/27/Unlocking-the-Power-Key-Features-of-MogDB/","permalink":"http://kamusis.github.io/2024/05/27/Unlocking-the-Power-Key-Features-of-MogDB/","excerpt":"","text":"Introduction to MogDBMogDB is a cutting-edge distributed relational database management system that offers an array of powerful features designed to meet the needs of modern businesses. With its high performance, availability, maintainability, compatibility, and AI capabilities, MogDB stands out as a top choice for database administrators, developers, and IT professionals. One of the key selling points of MogDB is its ability to deliver exceptional performance. This is achieved through various innovative features such as the Cost-Based Optimizer (CBO) optimizer, which intelligently chooses the most efficient execution plans for queries. Additionally, MogDB utilizes a vectorized engine that processes data in batches instead of row by row, resulting in significant performance improvements. The adaptive compression feature further enhances performance by reducing storage requirements and minimizing I&#x2F;O operations. In terms of availability, MogDB offers robust solutions to ensure uninterrupted access to critical data. It supports master-slave replication, allowing for automatic failover in case of primary node failure. Logical replication enables real-time data synchronization across multiple databases, while physical backup provides reliable data protection. Delayed replay allows for easy recovery from accidental data corruption or deletion. Maintaining a database can be complex and time-consuming. However, MogDB simplifies this process with its advanced maintainability features. The grey upgrade feature allows for seamless upgrades without interrupting service availability. Slow SQL diagnosis helps identify and optimize poorly performing queries, improving overall system efficiency. System KPI diagnosis provides insights into system health and performance metrics, enabling proactive maintenance and troubleshooting. Fault diagnosis helps pinpoint issues quickly and accurately. Compatibility is another area where MogDB excels. It supports various SQL features and ensures compatibility with popular database systems such as Oracle and MySQL. This makes it easier for organizations to migrate their existing applications or leverage their existing SQL knowledge without major modifications. MogDB also boasts impressive AI capabilities that set it apart from traditional databases. The AI4DB feature enables autonomous database operations, automating routine tasks and optimizing performance based on machine learning algorithms. DB4AI allows for database-driven AI, empowering organizations to leverage their data for advanced analytics and machine learning applications. Additionally, the ABO optimizer intelligently adapts query execution plans based on real-time data statistics, further enhancing performance. High Performance FeaturesMogDB is designed to deliver exceptional performance, ensuring that your database operations run smoothly and efficiently. With its cutting-edge features, MogDB offers unparalleled speed and optimization capabilities. One of the key high-performance features of MogDB is the Cost-Based Optimizer (CBO). This optimizer leverages advanced algorithms and statistical models to determine the most efficient execution plan for queries. By analyzing query statistics and data distribution, the CBO can make intelligent decisions on how to execute queries in the most optimal way. This results in faster query processing times and improved overall performance. In addition to the CBO optimizer, MogDB also utilizes a vectorized engine. This engine takes advantage of modern CPU architectures by performing operations on entire vectors of data at once, rather than processing individual elements sequentially. As a result, complex queries that involve large datasets can be executed more quickly and efficiently. Another feature that contributes to MogDB’s high performance is adaptive compression. This feature dynamically adjusts the level of compression applied to data based on its characteristics and usage patterns. By compressing data when it is not actively being accessed or modified, MogDB can reduce storage requirements and improve I&#x2F;O performance. When data needs to be accessed or modified, it is decompressed on-the-fly for seamless operations. Parallel query optimization is yet another powerful feature offered by MogDB. This feature allows queries to be divided into smaller tasks that can be executed simultaneously across multiple cores or nodes in a distributed environment. By leveraging parallelism, MogDB can significantly speed up query processing times for large datasets or complex queries. With these high-performance features combined, MogDB ensures that your database operations are lightning-fast and efficient. Whether you’re running simple CRUD operations or complex analytical queries, you can rely on MogDB to deliver exceptional performance every time. It’s worth noting that while these high-performance features greatly enhance the speed and efficiency of MogDB, they do not compromise on data integrity or reliability. MogDB is built with a strong focus on ACID (Atomicity, Consistency, Isolation, Durability) principles, ensuring that your data remains consistent and reliable even under high-performance workloads. High Availability FeaturesEnsuring high availability is crucial for any database management system, and MogDB excels in this aspect with its robust set of features. Let’s dive into the key high availability features that make MogDB a reliable choice for businesses. Master-slave replication for data redundancy MogDB offers master-slave replication, a powerful feature that enhances data redundancy and fault tolerance. With this feature, changes made to the master node are automatically replicated to one or more slave nodes. In the event of a failure or outage on the master node, one of the slave nodes can seamlessly take over as the new master, ensuring uninterrupted service availability. This replication mechanism not only provides data redundancy but also improves read scalability by allowing read operations to be distributed across multiple nodes. Logical replication for real-time data synchronization In addition to master-slave replication, MogDB supports logical replication, enabling real-time data synchronization between databases. This feature allows specific tables or even subsets of tables to be replicated from one database instance to another. By capturing and propagating changes at the logical level rather than replicating entire physical blocks, logical replication minimizes network traffic and reduces latency. It enables businesses to maintain up-to-date replicas of their databases for reporting purposes or offloading read-intensive workloads without impacting the performance of the primary database. Physical backup for data protection Data protection is paramount in any database system, and MogDB addresses this need through its physical backup feature. With physical backups, administrators can create full copies of their databases at a specific point in time. These backups capture both the schema and data files, ensuring comprehensive recovery options in case of hardware failures, user errors, or other catastrophic events. MogDB’s physical backup mechanism provides flexibility by allowing backups to be stored on different storage devices or even transferred to remote locations for disaster recovery purposes. Delayed replay for data recovery MogDB includes a delayed replay feature that allows administrators to recover data from a specific point in time. This feature is particularly useful in scenarios where accidental data deletions or modifications occur and need to be rolled back. By leveraging the transaction log, MogDB can replay changes up until a certain point, effectively restoring the database to its state prior to the incident. The delayed replay feature provides an additional layer of protection against human errors or malicious activities, ensuring that businesses can quickly recover from data-related incidents. In summary, MogDB offers a comprehensive set of high availability features that guarantee reliability and continuous operation for businesses. The master-slave replication ensures data redundancy and read scalability, while logical replication enables real-time data synchronization for reporting or offloading purposes. Physical backups and delayed replay provide robust data protection mechanisms, allowing administrators to recover from hardware failures or user errors effectively. With these high availability features, MogDB empowers organizations with the confidence that their critical databases will remain accessible and resilient even in the face of unexpected challenges. *[E-A-T]: Expertise, Authoritativeness, Trustworthiness Maintainability FeaturesMaintainability is a crucial aspect of any database management system, and MogDB excels in this area with its array of innovative features. These features are designed to ensure seamless system updates, optimize performance, monitor and analyze system KPIs, and resolve any potential faults. Let’s explore these maintainability features in detail. One of the standout maintainability features of MogDB is the grey upgrade capability. This feature allows for seamless system updates without interrupting ongoing operations. With grey upgrade, administrators can apply patches, upgrades, or even major version changes to MogDB without causing downtime or disrupting user access. This ensures that businesses can keep their databases up-to-date with the latest enhancements and security fixes while minimizing any potential disruptions to their operations. Another essential maintainability feature offered by MogDB is slow SQL diagnosis. Slow SQL queries can significantly impact database performance and user experience. MogDB addresses this issue by providing comprehensive tools for identifying and optimizing slow SQL queries. The system analyzes query execution plans, identifies bottlenecks, and suggests optimizations to improve query performance. By pinpointing problematic queries and optimizing them, administrators can enhance overall database performance and ensure smooth operation. System KPI diagnosis is another vital component of MogDB’s maintainability arsenal. Monitoring key performance indicators (KPIs) is crucial for understanding the health and efficiency of a database system. MogDB provides robust tools for monitoring and analyzing various KPIs such as CPU utilization, memory usage, disk I&#x2F;O, network traffic, and more. Administrators can set up custom alerts based on predefined thresholds to proactively identify any anomalies or potential issues before they impact the system’s performance or availability. In addition to diagnosing slow SQL queries and monitoring KPIs, MogDB also offers fault diagnosis capabilities. When an issue arises within the database system, it is essential to quickly identify the root cause and resolve it efficiently. MogDB provides advanced diagnostic tools that help administrators identify and troubleshoot various types of faults, including hardware failures, network issues, software bugs, or configuration problems. By quickly identifying and resolving faults, administrators can minimize downtime and ensure the continuous availability of their database system. Compatibility FeaturesMogDB offers a wide range of compatibility features that make it a versatile and flexible choice for database administrators, developers, and IT professionals. One of the key compatibility features is its support for various SQL features. With MogDB, you can leverage the full power of SQL and take advantage of advanced querying capabilities to meet your specific business needs. In addition to its support for SQL features, MogDB also provides seamless compatibility with Oracle databases. This compatibility feature allows for easy migration from Oracle to MogDB without any major disruptions or changes to your existing applications. The transition process is smooth and hassle-free, ensuring that you can quickly start benefiting from the high-performance and highly available nature of MogDB. Another compatibility feature offered by MogDB is its support for MySQL databases. This means that you can seamlessly integrate MogDB into your existing MySQL infrastructure without any major modifications. Whether you are running applications that rely on MySQL or have data stored in MySQL databases, MogDB ensures a seamless integration process, allowing you to leverage the advanced capabilities and performance enhancements provided by MogDB. The compatibility features of MogDB not only enable smooth transitions and integrations but also ensure that your existing applications continue to function seamlessly with minimal changes required. This level of compatibility reduces the effort and time required for migration or integration projects, allowing you to focus on other critical aspects of your business. With its comprehensive set of compatibility features, MogDB provides a robust solution that meets the diverse needs of different industries and applications. Whether you are working with complex SQL queries, migrating from Oracle databases, or integrating with MySQL infrastructure, MogDB offers the flexibility and reliability needed to ensure a successful deployment. AI CapabilitiesMogDB stands out among other distributed relational database management systems due to its advanced AI capabilities. These capabilities empower organizations to leverage the power of artificial intelligence for autonomous database operations, database-driven AI, and improved performance through the ABO optimizer. AI4DB for Autonomous Database OperationsWith MogDB’s AI4DB feature, organizations can enhance their operational efficiency by automating various database tasks. This includes automated performance tuning, query optimization, and workload management. The AI algorithms embedded within MogDB continuously monitor the system’s performance metrics and automatically adjust configurations to optimize resource allocation and improve overall database performance. AI4DB also plays a crucial role in self-healing mechanisms. It can detect anomalies or potential issues within the database environment and take proactive measures to resolve them before they impact critical business operations. By leveraging machine learning algorithms, MogDB can identify patterns in historical data and predict potential failures or bottlenecks, allowing administrators to take preventive actions. Furthermore, AI4DB enables intelligent data compression techniques that optimize storage utilization without compromising query performance. By analyzing data access patterns and applying advanced compression algorithms, MogDB reduces storage costs while ensuring fast data retrieval. DB4AI for Database-Driven AIMogDB’s DB4AI feature allows organizations to seamlessly integrate their databases with artificial intelligence applications. This empowers businesses to unlock valuable insights from their vast amounts of structured and unstructured data. By providing native support for popular machine learning frameworks such as TensorFlow and PyTorch, MogDB simplifies the process of training and deploying AI models directly within the database environment. This eliminates the need for complex data pipelines or costly data transfers between different systems. With DB4AI, organizations can leverage the full potential of their databases by performing real-time analytics on large volumes of data. They can train predictive models using historical data stored in MogDB and make accurate predictions based on real-time information ingested into the database. This enables businesses to make data-driven decisions faster and gain a competitive edge in today’s fast-paced market. ABO Optimizer for Improved PerformanceMogDB’s AI capabilities extend to its query optimization engine through the Adaptive Bitwise Optimization (ABO) optimizer. This innovative feature leverages machine learning techniques to intelligently optimize query execution plans based on historical performance data. The ABO optimizer continuously analyzes query patterns, execution statistics, and system resources to identify optimal query plans. By learning from past experiences, it can adaptively adjust execution strategies to improve overall query performance. This results in faster response times and more efficient resource utilization. Furthermore, the ABO optimizer reduces the need for manual tuning by automatically selecting the most appropriate join methods, access paths, and index usage based on the characteristics of each query. This simplifies database administration tasks and allows administrators to focus on higher-level optimizations rather than fine-tuning individual queries. ConclusionThe key features of MogDB make it a powerful and versatile option for database administrators, developers, and IT professionals. Its high performance capabilities, such as the CBO optimizer, vectorized engine, adaptive compression, and parallel query optimization, ensure that users can process large amounts of data quickly and efficiently. This is crucial in today’s fast-paced business environment where time is of the essence. Furthermore, MogDB offers high availability features that guarantee uninterrupted access to critical data. The master-slave replication, logical replication, physical backup, and delayed replay functionalities ensure that data is always accessible even in the event of system failures or disasters. This level of reliability instills confidence in users and provides peace of mind knowing that their data is safe. Maintainability is another key aspect of MogDB. With features like grey upgrade, slow SQL diagnosis, system KPI diagnosis, and fault diagnosis tools, administrators can easily identify and resolve issues within the database system. This streamlines maintenance processes and minimizes downtime for businesses. Compatibility with various SQL features as well as Oracle and MySQL compatibility allows for seamless integration with existing systems and applications. This eliminates the need for extensive modifications or rewrites when migrating from other database management systems to MogDB. In addition to these impressive features, MogDB also offers AI capabilities through AI4DB for autonomous database operations and DB4AI for database-driven AI. These advanced capabilities enable users to leverage artificial intelligence technologies within their databases to enhance performance and gain valuable insights from their data. Overall, MogDB stands out as a highly performant, highly available, easy-to-use distributed relational database management system with a wide range of features tailored to meet the needs of modern businesses. Its compatibility with existing systems and applications combined with its AI capabilities make it an attractive choice for organizations across industries. Whether you are a database administrator looking for improved performance or a developer seeking seamless integration options, MogDB has you covered. Trust in MogDB to unlock the power of your data.","categories":[],"tags":[{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"}]},{"title":"","slug":"The best practices for using partition table in PostgreSQL","date":"1970-01-01T00:00:00.000Z","updated":"2024-10-11T20:23:08.346Z","comments":true,"path":"1970/01/01/The best practices for using partition table in PostgreSQL/","permalink":"http://kamusis.github.io/1970/01/01/The%20best%20practices%20for%20using%20partition%20table%20in%20PostgreSQL/","excerpt":"","text":"The best practices for using partition table in PostgreSQLWhat is partition table?Partition table is a table that is divided into smaller parts. Each part is called a partition. Why use partition table? Performance: Partition table can improve the performance of the table. Because each partition can be stored in a separate file, the query can be executed faster. Maintenance: Partition table can make the maintenance of the table easier. Because each partition can be maintained separately, the maintenance of the table can be done faster. How to create a partition table? Create a partition table: 123456CREATE TABLE measurement ( city_id int not null, logdate date not null, peaktemp int, unitsales int) PARTITION BY RANGE (logdate); Create a partition table for each partition: 12345CREATE TABLE measurement_y2006m02 PARTITION OF measurement FOR VALUES FROM (&#x27;2006-02-01&#x27;) TO (&#x27;2006-03-01&#x27;);...CREATE TABLE measurement_y2007m11 PARTITION OF measurement FOR VALUES FROM (&#x27;2007-11-01&#x27;) TO (&#x27;2007-12-01&#x27;); Partition table typesThere are 3 basic types of partition table in PostgreSQL: List Partition: The table is partitioned by explicitly listing which key value(s) appear in each partition. Range Partition: The table is partitioned into “ranges” defined by a key column or set of columns, with no overlap between the ranges of values assigned to different partitions. Hash Partition: The table is partitioned by specifying a modulus and a remainder for each partition. Combine these 3 basic types of partition table, we can create more complex partition table. This is called Composite Partitioning. There are 4 types of composite partitioning: List-Range Partition: The partition is divided into several ranges.The step is: create table A partition by list, then create table A1 partition of A partition by range, then create table A1-1 partition of A1 Range-List Partition: The partition is divided into several ranges.The step is: create table A partition by range, then create table B of table A but partition by list. Range-Hash Partition: The partition is divided into several ranges. Hash-Range Partition: The partition is divided into several ranges. Data dictionary about partition table pg_partitioned_table: This table stores the information about the partitioned table. pg_partition_rule: This table stores the information about the partition rule. pg_partition_column: This table stores the information about the partition column. pg_partition_index: This table stores the information about the partition index. pg_partition_constraint: This table stores the information about the partition constraint. pg_partition_constraint_column: This table stores the information about the partition constraint column. pg_partition_constraint_index: This table stores the information about the partition constraint index.","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"VS Code","slug":"VS-Code","permalink":"http://kamusis.github.io/tags/VS-Code/"},{"name":"VSIX","slug":"VSIX","permalink":"http://kamusis.github.io/tags/VSIX/"},{"name":"Extension","slug":"Extension","permalink":"http://kamusis.github.io/tags/Extension/"},{"name":"Packaging","slug":"Packaging","permalink":"http://kamusis.github.io/tags/Packaging/"},{"name":"Windsurf","slug":"Windsurf","permalink":"http://kamusis.github.io/tags/Windsurf/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://kamusis.github.io/tags/PostgreSQL/"},{"name":"Database","slug":"Database","permalink":"http://kamusis.github.io/tags/Database/"},{"name":"DBOS","slug":"DBOS","permalink":"http://kamusis.github.io/tags/DBOS/"},{"name":"Operating System","slug":"Operating-System","permalink":"http://kamusis.github.io/tags/Operating-System/"},{"name":"Performance","slug":"Performance","permalink":"http://kamusis.github.io/tags/Performance/"},{"name":"MogDB","slug":"MogDB","permalink":"http://kamusis.github.io/tags/MogDB/"},{"name":"Oracle-Compatibility","slug":"Oracle-Compatibility","permalink":"http://kamusis.github.io/tags/Oracle-Compatibility/"},{"name":"Oracle","slug":"Oracle","permalink":"http://kamusis.github.io/tags/Oracle/"},{"name":"Database Migration","slug":"Database-Migration","permalink":"http://kamusis.github.io/tags/Database-Migration/"},{"name":"openGauss","slug":"openGauss","permalink":"http://kamusis.github.io/tags/openGauss/"},{"name":"MySQL","slug":"MySQL","permalink":"http://kamusis.github.io/tags/MySQL/"},{"name":"Partial Writes","slug":"Partial-Writes","permalink":"http://kamusis.github.io/tags/Partial-Writes/"},{"name":"WSL","slug":"WSL","permalink":"http://kamusis.github.io/tags/WSL/"},{"name":"Hexo","slug":"Hexo","permalink":"http://kamusis.github.io/tags/Hexo/"},{"name":"Github Pages","slug":"Github-Pages","permalink":"http://kamusis.github.io/tags/Github-Pages/"},{"name":"Windows","slug":"Windows","permalink":"http://kamusis.github.io/tags/Windows/"}]}